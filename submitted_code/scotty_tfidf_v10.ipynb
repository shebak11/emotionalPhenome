{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['label', 'outputs', 'text', 'encoding', 'prediction'])\n",
      "64301\n",
      "128\n",
      "64301\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from re import split\n",
    "data = pickle.load(open(\"emotype_v10.p\", \"rb\" ))\n",
    "print (data[0].keys())\n",
    "print (len(data))\n",
    "\n",
    "_, m = data[0]['encoding'].shape\n",
    "n = len(data)\n",
    "print (m)\n",
    "print (n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cluster_dic = {'addiction':0, 'anxiety':1, 'autism':2, \n",
    "                         'bipolar':3, 'conversation':4, 'depression':5, \n",
    "                         'happy':6, 'schizophrenia':7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_cluster_labels_with_ground_truth(data_dict):\n",
    "    \"\"\"Initialize dictionary of OPs with cluster labels according original topic. \n",
    "    \"\"\"\n",
    "    for i in range(len(data_dict)):\n",
    "        data_dict[i]['cluster'] = label_cluster_dic[data_dict[i]['label']]\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = initialize_cluster_labels_with_ground_truth(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I allow non important things to rule my mind. I have been spending a lot of time trying to get rock band controllers, setting up internet, organizing, etc.In reality I should be focusing on school. Why is it so was for me to obsess about nonimmediate issues. The most important things I should be worried about right now is my health, my education and financial situation.Yet, I write this while I am sitting in math class.  '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[2]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64301"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A potentially smoother way to get a dictionary initialized with all the words present in the corpus..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lexicon = set()\n",
    "# for i in range(len(data)):\n",
    "#     ith_phrase = data[i]['text'].strip().lower()  # Get text from ith OP\n",
    "#     ith_phrase.replace(\"'\", \"\")  # Get rid of apostrophes\n",
    "#     list_of_words_in_phrase = split(\"[^a-zA-Z]+\", ith_phrase)  \n",
    "#     lexicon.update(list_of_words_in_phrase)  # Add all of the text's words to lexicon\n",
    "# overall_word_freqs = {}.fromkeys(lexicon, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# print(len(lexicon))\n",
    "# some_words_in_the_lexicon = random.sample(lexicon, 10)  # Print 10 random words from our lexicon\n",
    "# print(some_words_in_the_lexicon)\n",
    "# print(len(overall_word_freqs.keys()))\n",
    "# print(overall_word_freqs['i'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64301\n",
      "['i', 'wonder', 'if', 'it', 'is', 'related', 'to', 'lack', 'of', 'object', 'permanence', 'i', 'feel', 'like', 'every', 'time', 'i', 'look', 'in', 'the', 'mirror', 'i', 'have', 'a', 'hard', 'time', 'wrapping', 'my', 'head', 'around', 'what', 'i', 'see', 'i', 'know', \"it's\", 'me', 'and', 'i', 'look', 'the', 'same', 'but', 'somehow', 'something', \"isn't\", 'right', 'almost', 'like', 'maybe', 'this', 'time', 'it', 'will', 'make', 'sense', 'and', 'stick', 'i', \"can't\", 'get', 'a', 'sense', 'of', 'sameness', 'sometimes', 'i', 'feel', 'ok', 'about', 'what', 'i', 'look', 'like', 'and', 'sometimes', 'i', \"can't\", 'stand', 'it', 'everyday', 'i', 'see', 'something', 'different', 'i', \"don't\", 'like', 'and', 'later', 'on', 'it', 'wont', 'bother', 'me', 'anymore', 'or', 'i', 'will', 'notice', 'something', 'else', 'that', 'i', \"don't\", 'like', 'either', 'way', 'it', 'just', 'never', 'really', 'seems', 'like', 'me', '']\n",
      "58576\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "all_text = []  # List to contain all of the words (our lexicon)\n",
    "for i in range(len(data)):\n",
    "    tmp = data[i]['text'].strip().lower()\n",
    "    all_text.append(split(\"[^a-zA-Z']+\", tmp))\n",
    "print(len(all_text))\n",
    "print(all_text[1])\n",
    "dic = {}\n",
    "for i in range(len(all_text)):  # i_th word in our lexicon\n",
    "    for j in all_text[i]:  #j\n",
    "        if j and dic.get(j, -1) != 0:\n",
    "            dic[j] = 0\n",
    "print(len(dic.keys()))\n",
    "print(dic['i'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'do'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list = list(dic.keys())\n",
    "word_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58576"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_text_with_cluster(cluster):\n",
    "    out = []\n",
    "    for i in range(len(data)):\n",
    "        if data[i]['cluster'] == cluster:\n",
    "            out.append(split(\"[^a-zA-Z']+\", data[i]['text'].strip().lower() ))\n",
    "    return out\n",
    "\n",
    "def count_word_of_cluster(texts):\n",
    "    out_dic = dic.fromkeys(dic,0)\n",
    "    for text in texts:\n",
    "        for word in text:\n",
    "            if word: out_dic[word] += 1\n",
    "    return out_dic\n",
    "\n",
    "def get_cluster_vectors(word_list, dic_cluster):\n",
    "    out = []\n",
    "    for word in word_list:  \n",
    "        out.append(dic_cluster[word])\n",
    "    return out\n",
    "\n",
    "def extract_vectors(cluster):\n",
    "    text_cluster = find_text_with_cluster(cluster)\n",
    "    dic_cluster = count_word_of_cluster(text_cluster)\n",
    "    vector_cluster = get_cluster_vectors(word_list,dic_cluster)\n",
    "    return vector_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = len(label_cluster_dic)  # We exlcude conversation\n",
    "num_words_in_lexicon = len(word_list)\n",
    "\n",
    "# Rows are clusters, columns are words, and elements are word frequencies in the cluster\n",
    "word_freq_mat = np.zeros((num_clusters, num_words_in_lexicon))  \n",
    "for cluster_name, cluster_label in label_cluster_dic.items():\n",
    "    cluster_freqs = np.array(extract_vectors(cluster_label))\n",
    "    word_freq_mat[cluster_label, :] = cluster_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters_with_word_present = np.sum(word_freq_mat > 0, axis=0, keepdims=True)\n",
    "tf_normalized_by_cluster = word_freq_mat / np.sum(word_freq_mat, axis=1, keepdims=True)\n",
    "idf = np.log(num_clusters / num_clusters_with_word_present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = tf_normalized_by_cluster * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "top_n_tfidf = np.argsort(tfidf, axis=1)[:, -n:]\n",
    "bottom_n_tfidf = np.argsort(tfidf, axis=1)[:, :n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aba',\n",
       " 'pddnos',\n",
       " 'sensory',\n",
       " 'neurotypical',\n",
       " 'workingout',\n",
       " 'raspergers',\n",
       " 'stim',\n",
       " 'stimming',\n",
       " 'autistics',\n",
       " 'nt',\n",
       " 'nonverbal',\n",
       " 'asperger',\n",
       " \"asperger's\",\n",
       " 'autistic',\n",
       " 'asd',\n",
       " 'nts',\n",
       " 'autism',\n",
       " 'aspergers',\n",
       " 'aspie',\n",
       " 'aspies']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word_list[i] for i in list(top_n_tfidf[label_cluster_dic['autism'], :])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['diagnosis',\n",
       " 'quetiapine',\n",
       " 'daylio',\n",
       " 'borderlines',\n",
       " 'pdoc',\n",
       " 'hypomanic',\n",
       " 'hypo',\n",
       " 'bp',\n",
       " 'abilify',\n",
       " 'lithium',\n",
       " 'manic',\n",
       " 'seroquel',\n",
       " 'mania',\n",
       " 'lamotrigine',\n",
       " 'latuda',\n",
       " 'dbt',\n",
       " 'bpd',\n",
       " 'hypomania',\n",
       " 'fp',\n",
       " 'lamictal']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word_list[i] for i in list(top_n_tfidf[label_cluster_dic['bipolar'], :])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['noose',\n",
       " 'relapsed',\n",
       " 'cared',\n",
       " 'pills',\n",
       " 'relapse',\n",
       " 'razor',\n",
       " 'loves',\n",
       " 'numb',\n",
       " 'killed',\n",
       " 'pussy',\n",
       " 'sertraline',\n",
       " 'hurts',\n",
       " 'empty',\n",
       " 'selfharm',\n",
       " 'harming',\n",
       " 'killing',\n",
       " 'alive',\n",
       " 'pathetic',\n",
       " 'worthless',\n",
       " 'scars']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word_list[i] for i in list(top_n_tfidf[label_cluster_dic['depression'], :])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['graph',\n",
       " 'testament',\n",
       " 'achieving',\n",
       " 'jeff',\n",
       " 'rohn',\n",
       " 'pomodoro',\n",
       " 'bennet',\n",
       " 'beforeafter',\n",
       " 'studybreak',\n",
       " 'moviesvideos',\n",
       " 'readout',\n",
       " 'materialization',\n",
       " 'pz',\n",
       " 'olsenvideos',\n",
       " 'youtuberbettermentbookclub',\n",
       " 'bennetcreated',\n",
       " \"shrink's\",\n",
       " 'selfpost',\n",
       " 'utcfriday',\n",
       " 'utc']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word_list[i] for i in list(top_n_tfidf[label_cluster_dic['happy'], :])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jew',\n",
       " 'barman',\n",
       " 'jewish',\n",
       " 'billy',\n",
       " 'replies',\n",
       " 'nun',\n",
       " 'pirate',\n",
       " 'officer',\n",
       " 'rabbi',\n",
       " 'genie',\n",
       " 'chief',\n",
       " 'paddy',\n",
       " 'exclaims',\n",
       " 'tl',\n",
       " 'priest',\n",
       " 'farmer',\n",
       " 'sir',\n",
       " 'blonde',\n",
       " 'johnny',\n",
       " 'bartender']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word_list[i] for i in list(top_n_tfidf[label_cluster_dic['conversation'], :])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['withdrawal',\n",
       " 'effexor',\n",
       " 'cymbalta',\n",
       " 'terrifies',\n",
       " 'interaction',\n",
       " 'nausea',\n",
       " 'pains',\n",
       " 'sweating',\n",
       " 'ssris',\n",
       " 'terrified',\n",
       " 'dizziness',\n",
       " 'paxil',\n",
       " 'citalopram',\n",
       " 'sertraline',\n",
       " 'palpitations',\n",
       " 'panicking',\n",
       " 'buspar',\n",
       " 'gad',\n",
       " 'sa',\n",
       " 'attack']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word_list[i] for i in list(top_n_tfidf[label_cluster_dic['anxiety'], :])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ignore this bit, I was playing around with weightings..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4402623770107763e-07"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_normalized_over_all_clusters = np.sum(word_freq_mat, axis=0, keepdims=True) / np.sum(word_freq_mat)\n",
    "np.min(tf_normalized_over_all_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-10\n",
    "gamma = np.log((tf_normalized_by_cluster +  eps) / tf_normalized_over_all_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_tfidf = tfidf * gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(np.min(tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00160351124941\n"
     ]
    }
   ],
   "source": [
    "print(np.max(tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "top_n_modified_tfidf = np.argsort(modified_tfidf, axis=1)[:, -n:]\n",
    "bottom_n_modified_tfidf = np.argsort(modified_tfidf, axis=1)[:, :n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sensory',\n",
       " 'meltdowns',\n",
       " 'neurotypical',\n",
       " 'pddnos',\n",
       " 'stim',\n",
       " 'raspergers',\n",
       " 'workingout',\n",
       " 'autistics',\n",
       " 'nt',\n",
       " 'stimming',\n",
       " 'nonverbal',\n",
       " 'asperger',\n",
       " \"asperger's\",\n",
       " 'autistic',\n",
       " 'nts',\n",
       " 'asd',\n",
       " 'autism',\n",
       " 'aspergers',\n",
       " 'aspie',\n",
       " 'aspies']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word_list[i] for i in list(top_n_modified_tfidf[label_cluster_dic['autism'], :])]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
