{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text', 'label', 'prediction', 'encoding', 'outputs'])\n",
      "52546\n",
      "256\n",
      "52546\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from re import split\n",
    "data = pickle.load(open(\"data/emotype_v7.p\", \"rb\" ))\n",
    "print (data[0].keys())\n",
    "print (len(data))\n",
    "\n",
    "_, m = data[0]['encoding'].shape\n",
    "n = len(data)\n",
    "print (m)\n",
    "print (n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cluster_dic = {'addiction':0, 'anxiety':1, 'autism':2, \n",
    "                         'bipolar':3, 'conversation':4, 'depression':5, \n",
    "                         'happy':6, 'schizophrenia':7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_cluster_labels_with_ground_truth(data_dict):\n",
    "    \"\"\"Initialize dictionary of OPs with cluster labels according original topic. \n",
    "    \"\"\"\n",
    "    for i in range(len(data_dict)):\n",
    "        data_dict[i]['cluster'] = label_cluster_dic[data_dict[i]['label']]\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = initialize_cluster_labels_with_ground_truth(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hey guys, (backstory) I am 21, F and I have had GAD and Panic Disorder for about 56 years, about 4 years ago I started waking up feeling like I was barely rested in the first place. I have tried meds and I cannot tolerate any SSRI's so far or SNRI's. I try to take melatonin, tried to get to bed earlier, but no matter those changes or my anxiety level I never tend to rest enough (even in lower periods of anxiety). I also have constant body aches everyday for about 67 years. I have quit caffeine months ago because it stopped being enough to make me feel awakeit increased my anxiety. Another weird side note is an hour or two before bed I suddenly am feeling less fatigued.Any advice would be appreciated, Thanks.TLDR: I am constantly exhausted and I have GAD advice would be appreciated!\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[2]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52546"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A potentially smoother way to get a dictionary initialized with all the words present in the corpus..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lexicon = set()\n",
    "# for i in range(len(data)):\n",
    "#     ith_phrase = data[i]['text'].strip().lower()  # Get text from ith OP\n",
    "#     ith_phrase.replace(\"'\", \"\")  # Get rid of apostrophes\n",
    "#     list_of_words_in_phrase = split(\"[^a-zA-Z]+\", ith_phrase)  \n",
    "#     lexicon.update(list_of_words_in_phrase)  # Add all of the text's words to lexicon\n",
    "# overall_word_freqs = {}.fromkeys(lexicon, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# print(len(lexicon))\n",
    "# some_words_in_the_lexicon = random.sample(lexicon, 10)  # Print 10 random words from our lexicon\n",
    "# print(some_words_in_the_lexicon)\n",
    "# print(len(overall_word_freqs.keys()))\n",
    "# print(overall_word_freqs['i'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52546\n",
      "['i', 'just', 'had', 'an', 'accident', 'in', 'a', 'company', 'vehicle', 'i', 'scraped', 'the', 'left', 'wing', 'mirror', 'on', 'a', 'bush', 'driving', 'down', 'a', 'narrow', 'road', 'in', 'the', 'dark', 'the', 'mirror', 'was', 'intact', 'but', 'the', 'back', 'panel', 'of', 'the', 'mirror', 'came', 'off', 'this', 'was', 'a', 'brand', 'new', 'vehicle', 'and', 'tomorrow', \"i'm\", 'going', 'to', 'have', 'to', 'explain', 'what', 'happened', 'to', 'my', 'boss', 'this', \"isn't\", 'the', 'first', 'incident', \"i've\", 'been', 'in', 'either', 'i', \"don't\", 'even', 'want', 'to', 'drive', \"it's\", 'not', 'part', 'of', 'my', 'job', 'description', 'but', 'noone', 'else', 'will', 'do', 'it', 'noone', 'else', 'was', 'stupid', 'enough', 'i', 'guess', 'i', 'just', 'want', 'to', 'hide', 'in', 'a', 'corner', 'and', 'die', 'thinking', 'about', 'it', 'i', 'feel', 'like', 'such', 'a', 'useless', 'stupid', 'idiot', \"it's\", 'past', 'midnight', 'here', 'and', 'i', \"can't\", 'sleep', 'from', 'worry', '']\n",
      "58236\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "all_text = []  # List to contain all of the words (our lexicon)\n",
    "for i in range(len(data)):\n",
    "    tmp = data[i]['text'].strip().lower()\n",
    "    all_text.append(split(\"[^a-zA-Z']+\", tmp))\n",
    "print(len(all_text))\n",
    "print(all_text[1])\n",
    "dic = {}\n",
    "for i in range(len(all_text)):  # i_th word in our lexicon\n",
    "    for j in all_text[i]:  #j\n",
    "        if j and dic.get(j, -1) != 0:\n",
    "            dic[j] = 0\n",
    "print(len(dic.keys()))\n",
    "print(dic['i'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'im\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list = list(dic.keys())\n",
    "word_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58236"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_text_with_cluster(cluster):\n",
    "    out = []\n",
    "    for i in range(len(data)):\n",
    "        if data[i]['cluster'] == cluster:\n",
    "            out.append(split(\"[^a-zA-Z']+\", data[i]['text'].strip().lower() ))\n",
    "    return out\n",
    "\n",
    "def count_word_of_cluster(texts):\n",
    "    out_dic = dic.fromkeys(dic,0)\n",
    "    for text in texts:\n",
    "        for word in text:\n",
    "            if word: out_dic[word] += 1\n",
    "    return out_dic\n",
    "\n",
    "def get_cluster_vectors(word_list, dic_cluster):\n",
    "    out = []\n",
    "    for word in word_list:  \n",
    "        out.append(dic_cluster[word])\n",
    "    return out\n",
    "\n",
    "def extract_vectors(cluster):\n",
    "    text_cluster = find_text_with_cluster(cluster)\n",
    "    dic_cluster = count_word_of_cluster(text_cluster)\n",
    "    vector_cluster = get_cluster_vectors(word_list,dic_cluster)\n",
    "    return vector_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = len(label_cluster_dic)  # We exlcude conversation\n",
    "num_words_in_lexicon = len(word_list)\n",
    "\n",
    "# Rows are clusters, columns are words, and elements are word frequencies in the cluster\n",
    "word_freq_mat = np.zeros((num_clusters, num_words_in_lexicon))  \n",
    "for cluster_name, cluster_label in label_cluster_dic.items():\n",
    "    cluster_freqs = np.array(extract_vectors(cluster_label))\n",
    "    word_freq_mat[cluster_label, :] = cluster_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters_with_word_present = np.sum(word_freq_mat > 0, axis=0, keepdims=True)\n",
    "tf_normalized_by_cluster = word_freq_mat / np.sum(word_freq_mat, axis=1, keepdims=True)\n",
    "idf = np.log(num_clusters / num_clusters_with_word_present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = tf_normalized_by_cluster * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "top_n_tfidf = np.argsort(tfidf, axis=1)[:, -n:]\n",
    "bottom_n_tfidf = np.argsort(tfidf, axis=1)[:, :n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sensory',\n",
       " 'pddnos',\n",
       " 'meltdowns',\n",
       " 'neurotypical',\n",
       " 'stim',\n",
       " 'workingout',\n",
       " 'raspergers',\n",
       " 'autistics',\n",
       " 'nt',\n",
       " 'stimming',\n",
       " 'nonverbal',\n",
       " 'asperger',\n",
       " \"asperger's\",\n",
       " 'autistic',\n",
       " 'nts',\n",
       " 'asd',\n",
       " 'autism',\n",
       " 'aspergers',\n",
       " 'aspie',\n",
       " 'aspies']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word_list[i] for i in list(top_n_tfidf[label_cluster_dic['autism'], :])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['geodon',\n",
       " 'diagnosis',\n",
       " 'psychotic',\n",
       " 'borderlines',\n",
       " 'pdoc',\n",
       " 'hypomanic',\n",
       " 'hypo',\n",
       " 'lithium',\n",
       " 'abilify',\n",
       " 'seroquel',\n",
       " 'bp',\n",
       " 'manic',\n",
       " 'mania',\n",
       " 'lamotrigine',\n",
       " 'latuda',\n",
       " 'dbt',\n",
       " 'bpd',\n",
       " 'hypomania',\n",
       " 'fp',\n",
       " 'lamictal']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word_list[i] for i in list(top_n_tfidf[label_cluster_dic['bipolar'], :])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['committing',\n",
       " 'relapsed',\n",
       " 'pills',\n",
       " 'relapse',\n",
       " 'razor',\n",
       " 'cared',\n",
       " 'loves',\n",
       " 'numb',\n",
       " 'killed',\n",
       " 'sertraline',\n",
       " 'pussy',\n",
       " 'hurts',\n",
       " 'empty',\n",
       " 'harming',\n",
       " 'selfharm',\n",
       " 'killing',\n",
       " 'alive',\n",
       " 'pathetic',\n",
       " 'worthless',\n",
       " 'scars']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word_list[i] for i in list(top_n_tfidf[label_cluster_dic['depression'], :])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['graph',\n",
       " 'testament',\n",
       " 'achieving',\n",
       " 'jeff',\n",
       " 'rohn',\n",
       " 'bennet',\n",
       " 'beforeafter',\n",
       " 'pomodoro',\n",
       " 'materialization',\n",
       " 'youtuberbettermentbookclub',\n",
       " 'readout',\n",
       " 'pz',\n",
       " \"shrink's\",\n",
       " 'moviesvideos',\n",
       " 'studybreak',\n",
       " 'bennetcreated',\n",
       " 'olsenvideos',\n",
       " 'selfpost',\n",
       " 'utcfriday',\n",
       " 'utc']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word_list[i] for i in list(top_n_tfidf[label_cluster_dic['happy'], :])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ignore this bit, I was playing around with weightings..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf_normalized_over_all_clusters = np.sum(word_freq_mat, axis=0, keepdims=True) / np.sum(word_freq_mat)\n",
    "np.min(tf_normalized_over_all_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-10\n",
    "gamma = np.log((tf_normalized_by_cluster +  eps) / tf_normalized_over_all_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_tfidf = tfidf * gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(np.min(tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00160351124941\n"
     ]
    }
   ],
   "source": [
    "print(np.max(tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "top_n_modified_tfidf = np.argsort(modified_tfidf, axis=1)[:, -n:]\n",
    "bottom_n_modified_tfidf = np.argsort(modified_tfidf, axis=1)[:, :n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sensory',\n",
       " 'meltdowns',\n",
       " 'neurotypical',\n",
       " 'pddnos',\n",
       " 'stim',\n",
       " 'raspergers',\n",
       " 'workingout',\n",
       " 'autistics',\n",
       " 'nt',\n",
       " 'stimming',\n",
       " 'nonverbal',\n",
       " 'asperger',\n",
       " \"asperger's\",\n",
       " 'autistic',\n",
       " 'nts',\n",
       " 'asd',\n",
       " 'autism',\n",
       " 'aspergers',\n",
       " 'aspie',\n",
       " 'aspies']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word_list[i] for i in list(top_n_modified_tfidf[label_cluster_dic['autism'], :])]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
