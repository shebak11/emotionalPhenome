{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook includes all the steps for going from the RS files to the files you will need for training and testing your model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import glob\n",
    "from carsonNLP.embedding import Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "## How to allocate the RS_YEAR-DAY files, and what are the intermediate files\n",
    "\n",
    "Put all the RS files into one folder \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_RS = \"RS\"\n",
    "all_filepaths = glob.glob(path_to_RS+'/RS*')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The old class to subreddit mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label2subreddit_dict =  {\n",
    "'conversation' = ['CasualConversation', 'relationships','AskReddit','Jokes', 'CoolStoryBro', \n",
    "                'stupidpeople','Showerthoughts','DoesAnybodyElse','changemyview','Foodforthought',\n",
    "                'announcements','funny','AskReddit','todayilearned','IAmA','news'],\n",
    "'happy' = ['happy','UpliftingNews', 'feelgood', 'goodfeelings', 'ThankYou', 'GetMotivated', \n",
    "         'DecidingToBeBetter', 'benicetopeople' ,'AccomplishedToday','InspirationalMoments'],\n",
    "'anxiety' = ['Anxiety','socialanxiety','AnxietyPanic'],\n",
    "'addiction' = ['addiction','opiates','OpiatesRecovery','cripplingalcoholism'],\n",
    "'depression' = ['depression', 'StopSelfHarm', 'selfharm', 'SuicideWatch'],\n",
    "'bipolar' = ['BPD','bipolar','BipolarSOs'],\n",
    "'autism' = ['autism','aspergers'],\n",
    "'schizophrenia' = ['schizophrenia']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## choose unacceptable strings\n",
    "do not use submission/posts that have these unacceptable strings in their 'selftext' field\n",
    "## choose a minimum and maximum string length for inclusion\n",
    "this is the whole string including all characters and spaces, make this range the range that you think meaningful posts come in, not the range that will fit into a minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unacceptable = ['[removed]','[deleted]']\n",
    "\n",
    "min_chars = 10\n",
    "max_chars = 10000\n",
    "\n",
    "'''   \n",
    "label2subreddit_dict = {\n",
    "\n",
    "'conversation' : ['CasualConversation', 'relationships','AskReddit', 'CoolStoryBro', \n",
    "                'stupidpeople','Showerthoughts','DoesAnybodyElse','changemyview','Foodforthought',\n",
    "                'announcements','AskReddit','todayilearned','IAmA','news','gossip'],\n",
    "\n",
    "'jokes': ['jokes','funny'],\n",
    "    \n",
    "'happy' : ['happy','UpliftingNews', 'feelgood', 'goodfeelings', 'ThankYou', \n",
    "           'AccomplishedToday','InspirationalMoments',\n",
    "           'faithinhumanity','gratitude','goodnews','MadeMeSmile','moodboost',\n",
    "           'optimism','palatecleanser','positivity','thankyou','wherepeoplecare',\n",
    "            'DecidingToBeBetter','small_act_of_kindness_from_a_friend', 'randomactsofkindness'],\n",
    "\n",
    "'anxiety' : ['Anxiety','socialanxiety','AnxietyPanic'],\n",
    "\n",
    "'addiction' : ['addiction','opiates','OpiatesRecovery','cripplingalcoholism'],\n",
    "\n",
    "'depression' : ['depression'], \n",
    "    \n",
    " 'selfharm'  : ['StopSelfHarm', 'selfharm', 'SuicideWatch'],\n",
    "\n",
    "'bipolar' : ['BPD','bipolar','BipolarSOs'],\n",
    "\n",
    "'autism' : ['autism','aspergers'],\n",
    "\n",
    "'schizophrenia' : ['schizophrenia']\n",
    "\n",
    "}\n",
    "'''\n",
    "\n",
    "label2subreddit_dict =  {\n",
    "'conversation' : ['CasualConversation', 'relationships','AskReddit','Jokes', 'CoolStoryBro', \n",
    "                'stupidpeople','Showerthoughts','DoesAnybodyElse','changemyview','Foodforthought',\n",
    "                'announcements','funny','AskReddit','todayilearned','IAmA','news'],\n",
    "'happy' : ['happy','UpliftingNews', 'feelgood', 'goodfeelings', 'ThankYou', 'GetMotivated', \n",
    "         'DecidingToBeBetter', 'benicetopeople' ,'AccomplishedToday','InspirationalMoments'],\n",
    "'anxiety' : ['Anxiety','socialanxiety','AnxietyPanic'],\n",
    "'addiction' : ['addiction','opiates','OpiatesRecovery','cripplingalcoholism'],\n",
    "'depression' : ['depression', 'StopSelfHarm', 'selfharm', 'SuicideWatch'],\n",
    "'bipolar' : ['BPD','bipolar','BipolarSOs'],\n",
    "'autism' : ['autism','aspergers'],\n",
    "'schizophrenia' : ['schizophrenia']\n",
    "}\n",
    "\n",
    "subreddit2label_dict = {}\n",
    "for key, value in label2subreddit_dict.items():\n",
    "    for subreddit in value:\n",
    "        subreddit2label_dict[subreddit.lower()]=key.lower()\n",
    "\n",
    "data_count = dict.fromkeys(label2subreddit_dict, 0)\n",
    "\n",
    "subreddit_list = []\n",
    "for key, value in label2subreddit_dict.items():\n",
    "    subreddit_list.extend([v.lower() for v in value])\n",
    "    \n",
    "happy_subreddit_dict = {}\n",
    "for happy_subreddit in label2subreddit_dict['happy']:\n",
    "    happy_subreddit_dict[happy_subreddit.lower()] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "default - rumination \n",
    "\n",
    "salience - axious avoidance\n",
    "\n",
    "negative affect - negative bias, threat dysregulation\n",
    "\n",
    "positive affect   - anhedonia (reward hypoactivation), context insensitivity\n",
    "\n",
    "attention - inatttention\n",
    "\n",
    "congnitive control - cognitive dyscontrol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_folder_path = 'class_folder_path3'\n",
    "\n",
    "if not os.path.exists(class_folder_path):\n",
    "    os.mkdir(class_folder_path)\n",
    "    \n",
    "for label in label2subreddit_dict.keys():\n",
    "    label_doc_path = os.path.join(class_folder_path,label+'.txt')\n",
    "    if not os.path.exists(label_doc_path):\n",
    "        file = open(label_doc_path,'w')\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The raw text is placed, the function normalize_string not used until the embedding and data_dict stages\n",
    "\n",
    "### I made the decision to use add the title to the beginning of the post and use only the title when the post is not in range "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RS/RS_2017-06\n",
      "RS/RS_2017-07\n",
      "RS/RS_2017-08\n",
      "RS/RS_2017-09\n",
      "RS/RS_2017-10\n",
      "RS/RS_2017-11\n",
      "RS/RS_2017-12\n",
      "RS/RS_2018-01\n",
      "RS/RS_2018-02\n"
     ]
    }
   ],
   "source": [
    "# this for loop takes more 1 - 2 hours to run on 8 RS files\n",
    "for filepaths in all_filepaths:\n",
    "    print(filepaths)\n",
    "    with open(filepaths, buffering=100) as f:\n",
    "        for row in f:\n",
    "            row = json.loads(row)\n",
    "            if row['selftext'] not in unacceptable \\\n",
    "            and 'subreddit' in row.keys() \\\n",
    "            and row['subreddit'].lower() in subreddit_list:\n",
    "                label = subreddit2label_dict[row['subreddit'].lower()]\n",
    "                label_doc_path = os.path.join(class_folder_path,label+'.txt')\n",
    "                file = open(label_doc_path,'a')\n",
    "                if len(row['selftext'].strip()) in range(min_chars,max_chars):\n",
    "                    file.write(row['title'] + ' , ' + row['selftext']+\"\\n\") \n",
    "                else:\n",
    "                    file.write(row['title']+\"\\n\")\n",
    "                file.close()\n",
    "                \n",
    "                if label == 'happy':\n",
    "                    happy_subreddit_dict[row['subreddit'].lower()].append(row['title'] + ' , ' + row['selftext'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index of first validation sample -1100\n",
      "index of first test sample -1000\n",
      "addiction 51116\n",
      "anxiety 58900\n",
      "autism 19887\n",
      "bipolar 43710\n",
      "conversation 2047037\n",
      "depression 130572\n",
      "happy 47216\n",
      "jokes 466398\n",
      "schizophrenia 10148\n",
      "selfharm 53799\n"
     ]
    }
   ],
   "source": [
    "max_len = 1000\n",
    "min_len = 10\n",
    "path_to_folder_of_text = 'class_folder_path2'\n",
    "all_filenames = glob.glob(path_to_folder_of_text+'/*.txt')\n",
    "\n",
    "vocabClass = Vocabulary()\n",
    "\n",
    "training_dict = {}\n",
    "validation_dict = {}\n",
    "test_dict = {}\n",
    "all_data = {}\n",
    "all_categories = []\n",
    "\n",
    "# training validation split here, the val and test set are balanced because the same number of samples are taken\n",
    "# from the end of each document class to be placed into this set, but the training set might still be unbalanced\n",
    "num_test = 1000 # number of test samples per class\n",
    "num_validation = 100 # number of validation samples per class\n",
    "last_train = -num_test - num_validation\n",
    "print(\"index of first validation sample\",last_train)\n",
    "last_validation = -num_test\n",
    "print(\"index of first test sample\",last_validation)\n",
    "\n",
    "for filename in all_filenames:\n",
    "    \n",
    "    category = filename.split('/')[-1].split('.')[0] # get label name from file\n",
    "    all_categories.append(category) # add labe to list of labels\n",
    "    lines = open(filename).read().strip().split('\\n') # split based on the newline delimiter\n",
    "    lines = [vocabClass.normalize_string(line) for line in lines if len(line) in range(min_len,max_len)] \n",
    "    print(category,len(lines))\n",
    "    all_data[category] = lines\n",
    "    training_dict[category] = lines[:last_train]\n",
    "    validation_dict[category] = lines[last_train:last_validation]\n",
    "    test_dict[category] = lines[last_validation:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emotrvaltest = (training_dict, validation_dict, test_dict, all_data, all_categories)\n",
    "pickle.dump(emotrvaltest, open(\"trvaltest/emotrvaltest_human.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Vocabulary\n",
    "\n",
    "our initial vocabulary had  18024 tokens, we only used lowercase words, tha can be changed easily by changing\n",
    "\n",
    "s = self.unicode_to_ascii(s.lower()) to s = self.unicode_to_ascii(s)\n",
    "\n",
    "only a-zA-Z0-9,.!? are kept "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 words\n"
     ]
    }
   ],
   "source": [
    "import torchtext.vocab as vocab\n",
    "glove = vocab.GloVe(name='6B', dim=100) # available in 100\n",
    "print('Loaded {} words'.format(len(glove.itos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trimming...\n",
      "keep_words 30613 / 324298 = 0.0944\n",
      "building embedding from glove...\n",
      "finished embeddings\n"
     ]
    }
   ],
   "source": [
    "vocabClass = Vocabulary()\n",
    "min_word_count = 30\n",
    "path_to_folder_of_text = 'class_folder_path2'\n",
    "index2word, word2index, embedding = vocabClass.makeEmbedding(min_word_count, glove, path_to_folder_of_text)\n",
    "dicts_embed = (index2word, word2index, embedding)\n",
    "pickle.dump(dicts_embed, open(\"embeddings/dicts_embed_min30.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this dont , ? ! thats youre no\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = vocabClass.normalize_string(\"      This    don't,?! that's you're \\\"no\\\" \")\n",
    "print(st)\n",
    "\"\\\" no \\\"\" in st"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
