{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are using PyTorch version  0.4.1\n",
      "no GPUs detected\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torchtext.vocab as vocab\n",
    "from carsonNLP.embedding import Vocabulary\n",
    "from carsonNLP.string_token_functions import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print('you are using PyTorch version ',torch.__version__)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    use_cuda = True\n",
    "    print('you have at least 1 GPU')\n",
    "else:\n",
    "    use_cuda = False\n",
    "    print('no GPUs detected')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "group donates 90 defibrillators to indiana state police\n"
     ]
    }
   ],
   "source": [
    "f = open(\"trvaltest/trvaltest_10cl_5-100words_v2.p\", \"rb\")\n",
    "# f = open( \"trvaltest/emotrvaltest6.p\", \"rb\")\n",
    "training_dict, validation_dict, test_dict, all_data, all_categories  \\\n",
    "= pickle.load(f, encoding=\"utf-8\")\n",
    "print(len(validation_dict['autism']))\n",
    "print(validation_dict['happy'][4])\n",
    "\n",
    "all_categories = ['addiction',\n",
    "                  'anxiety',\n",
    "                  'autism',\n",
    "                  'bipolar',\n",
    "                  'conversation',\n",
    "                  'depression',\n",
    "                  'happy',\n",
    "                  'jokes',\n",
    "                  'schizophrenia',\n",
    "                  'selfharm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word, word2index, embedding  = pickle.load(open(\"embeddings/dicts_embed_min40_folder4.p\", \"rb\"))\n",
    "vocabClass = Vocabulary()\n",
    "vocabClass.index2word = index2word\n",
    "vocabClass.word2index = word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        #self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "                       nn.Linear(hidden_size*2, hidden_size),\n",
    "                       #nn.BatchNorm1d(num_features=1), # NEW\n",
    "                       nn.PReLU(),\n",
    "                       nn.Linear(hidden_size, 1)\n",
    "                    )\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        seq_len = encoder_outputs.size(0)\n",
    "        this_batch_size = encoder_outputs.size(1)\n",
    "        # print(' hidden.size(), encoder_outputs.size()', hidden.size(), encoder_outputs.size()) \n",
    "        # torch.Size([batch_size, hidden_size]) torch.Size([seq_len, batch_size, hidden_size])\n",
    "        # Create variable to store attention energies\n",
    "        attn_energies = Variable(torch.zeros(this_batch_size, seq_len)) # B x S\n",
    "\n",
    "        if use_cuda:\n",
    "            attn_energies = attn_energies.cuda()\n",
    "\n",
    "        # For each batch of encoder outputs\n",
    "        for b in range(this_batch_size):\n",
    "            # Calculate energy for each encoder output\n",
    "            for i in range(seq_len):\n",
    "                #attn_energies[b, i] = self.score(hidden[:, b], encoder_outputs[i, b].unsqueeze(0))\n",
    "                attn_energies[b, i] = self.score(hidden[b], encoder_outputs[i, b])\n",
    "\n",
    "        attn_weights = F.softmax(attn_energies,dim=1).unsqueeze(1) # batch_size,1,seq_len\n",
    "        \n",
    "        return attn_weights \n",
    "    \n",
    "    def score(self, hidden, encoder_output):\n",
    "        #print(hidden.size(), encoder_output.size())\n",
    "        #concat = torch.cat((hidden, encoder_output), 1)\n",
    "        concat = torch.cat((hidden, encoder_output))\n",
    "        energy = self.fc1(concat)\n",
    "        return energy\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, embedding, output_size, num_layers = 3, bidirectional = False, \n",
    "                 train_embedding = True , dropout = 0.0):\n",
    "        \n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.dropout = nn.Dropout(p=dropout) # p â€“ probability of an element to be zeroed. Default: 0.5\n",
    "        \n",
    "        embedding = torch.from_numpy(embedding).float()\n",
    "        \n",
    "        if use_cuda:\n",
    "            embedding.cuda()\n",
    "        \n",
    "        self.embedding = nn.Embedding(embedding.shape[0], embedding.shape[1])\n",
    "        self.embedding.weight = nn.Parameter(embedding, requires_grad=train_embedding)\n",
    "        self.gru = nn.GRU(embedding.shape[1], hidden_size, num_layers, \n",
    "                          bidirectional=bidirectional, dropout = dropout)\n",
    "        \n",
    "        if bidirectional:\n",
    "            num_directions = 2\n",
    "        else:\n",
    "            num_directions = 1\n",
    "        \n",
    "        # make the initial hidden state learnable as well \n",
    "        hidden0 = torch.zeros(self.num_layers*num_directions, 1, self.hidden_size)\n",
    "        self.hidden0 = nn.Parameter(hidden0, requires_grad=True)\n",
    "        \n",
    "        self.num_cells = num_layers*num_directions\n",
    "        \n",
    "        self.fc_concat = nn.Sequential(\n",
    "                             nn.Linear(hidden_size * 2, hidden_size),\n",
    "                             nn.BatchNorm1d(num_features=self.hidden_size)\n",
    "                             ) \n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "                       nn.Linear(self.hidden_size,self.hidden_size),\n",
    "                       nn.BatchNorm1d(num_features=self.hidden_size),\n",
    "                       )\n",
    "        \n",
    "        self.out = nn.Linear(hidden_size,output_size)\n",
    "        self.prelu = nn.PReLU()\n",
    "        self.attn = Attn(hidden_size)\n",
    "\n",
    "    def forward(self, input_seqs):\n",
    "        \n",
    "        batch_size = input_seqs.size(1)\n",
    "        hidden = self.hidden0.repeat(1, batch_size, 1)\n",
    "        self.embedded = self.embedding(input_seqs)\n",
    "        encoder_outputs, last_seq_hidden = self.gru(self.embedded, hidden)\n",
    "        last_hidden = last_seq_hidden[-1] \n",
    "        \n",
    "        attn_weights = self.attn(last_hidden, encoder_outputs) #  batch_size,1,seq_len\n",
    "        \n",
    "        # output of GRU (seq_len, batch_size, hidden_size) -> (batch_size, seq_len, hidden_size)\n",
    "        encoder_outputs_bsh = encoder_outputs.transpose(0, 1)\n",
    "        \n",
    "        # bmm does operation (b,1,s).bmm(b,s,h) = (b,1,h)\n",
    "        context = torch.bmm(attn_weights,encoder_outputs_bsh) # should be  # B x S=1 x N\n",
    "        \n",
    "        # Attentional vector using the RNN hidden state and context vector concatenated together \n",
    "        context = context.squeeze(1)       # B x S=1 x H -> Batch Size x Hidden Size\n",
    "        concat_input = torch.cat((last_hidden, context), 1) # both should be batch_size x hidden_size\n",
    "        \n",
    "        concat_output = F.tanh(self.fc_concat(concat_input)) # <hidden_size>\n",
    "\n",
    "        fc1 = self.fc1(concat_output)\n",
    "\n",
    "        output = self.out(self.dropout(fc1))\n",
    "        #output = self.out(self.dropout(self.prelu(fc1)))\n",
    "        \n",
    "        return output, fc1 , attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_categories = 10\n",
      "example of category:  bipolar\n",
      "take your metal once a day friend of mine linked this on fb for me\n",
      "['addiction', 'anxiety', 'autism', 'bipolar', 'conversation', 'depression', 'happy', 'jokes', 'schizophrenia', 'selfharm']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19479"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_categories = len(all_categories)\n",
    "print('n_categories =', n_categories)\n",
    "category = random.choice(all_categories)\n",
    "print('example of category: ',category)\n",
    "print(validation_dict[category][0])\n",
    "print(all_categories)\n",
    "vocabClass.word2index['thats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 256\n",
    "num_layers = 3\n",
    "bidirectional = False\n",
    "\n",
    "rnn = RNN(n_hidden, embedding, n_categories, num_layers = num_layers, \n",
    "          bidirectional = bidirectional, dropout = 0.2)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if use_cuda:\n",
    "    rnn = rnn.cuda()\n",
    "\n",
    "name = 'trvaltest_10cl_5-100w_embed_min40_v5_' + \\\n",
    "        str(n_hidden) + '_' + str(num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_validation(vocabClass, verbose=True):\n",
    "    rnn.train(False)\n",
    "    count = 0\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    \n",
    "    for category in validation_dict.keys():\n",
    "        category_count = 0\n",
    "        category_correct = 0\n",
    "        category_tensor = Variable(torch.LongTensor([all_categories.index(category)]))\n",
    "        if use_cuda:\n",
    "            category_tensor = category_tensor.cuda()\n",
    "        for line in validation_dict[category]:\n",
    "            line_tensor = Variable(torch.LongTensor(indexesFromSentence(vocabClass, line))).view(-1,1)\n",
    "            #print(line_tensor.size())\n",
    "            if use_cuda:\n",
    "                line_tensor = line_tensor.cuda()\n",
    "            output, vector_rep, attn_wts = rnn(line_tensor)\n",
    "            loss = criterion(output, category_tensor)\n",
    "            total_loss += loss\n",
    "            category_count += 1\n",
    "            count += 1\n",
    "            topv, topi = output.data.topk(1, 1, True)\n",
    "            category_index = topi[0][0]\n",
    "            if category == all_categories[category_index]:\n",
    "                category_correct += 1\n",
    "                total_correct += 1\n",
    "        if verbose:\n",
    "            print(\"Category {} accuracy = {:.2f}\".format(category, float(category_correct) / category_count))\n",
    "    avg_loss = float(total_loss)/count\n",
    "    accuracy = float(total_correct)/count\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def predict_line(line, vocabClass, rnn):\n",
    "        line_tensor = Variable(torch.LongTensor(indexesFromSentence(vocabClass, line))).view(-1,1)\n",
    "        if use_cuda:\n",
    "            line_tensor = line_tensor.cuda()\n",
    "        rnn.train(False)\n",
    "        output, rep, attn_wts = rnn(line_tensor)\n",
    "        guess, guess_i = category_from_output(output)\n",
    "        return guess, output, attn_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trvaltest_10cl_5-100w_embed_min40_v5_256_3'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Scotty\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category addiction accuracy = 0.70\n",
      "Category happy accuracy = 0.65\n",
      "Category selfharm accuracy = 0.58\n",
      "Category schizophrenia accuracy = 0.59\n",
      "Category bipolar accuracy = 0.44\n",
      "Category anxiety accuracy = 0.67\n",
      "Category depression accuracy = 0.34\n",
      "Category autism accuracy = 0.42\n",
      "Category jokes accuracy = 0.84\n",
      "Category conversation accuracy = 0.75\n",
      "avg_val_loss 1.9858 ,  val_accuracy 0.5980\n"
     ]
    }
   ],
   "source": [
    "rnn.load_state_dict(torch.load(\"modelstate/\" + name + \"_cpu.pth\"))\n",
    "avg_val_loss, val_accuracy = eval_validation(vocabClass)\n",
    "print('avg_val_loss %.4f ,  val_accuracy %.4f' % (avg_val_loss, val_accuracy)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
