{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main.pynb\n",
    "\n",
    "using PyTorch version  0.4.0\n",
    "\n",
    "This is the main notebook that will be dealing with the portion of the project currently being tested or under development.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are using PyTorch version  0.4.0\n",
      "you have at least 1 GPU\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torchtext.vocab as vocab\n",
    "from carsonNLP.embedding import Vocabulary\n",
    "from carsonNLP.string_token_functions import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print('you are using PyTorch version ',torch.__version__)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    use_cuda = True\n",
    "    print('you have at least 1 GPU')\n",
    "else:\n",
    "    use_cuda = False\n",
    "    print('no GPUs detected')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "\n",
    "Currently the data is stored in memory in dictionaries for the training validation and test set\n",
    "where the key is the class and the value is a list of all the strings corresponding to that class\n",
    "\n",
    "For the notebook that generates this dataset, see the data_preprocess folder and RS_to_data_dict.ipynb\n",
    "\n",
    "emotrvaltest9.p\n",
    "index of first validation sample -1100\n",
    "index of first test sample -1000\n",
    "addiction 71785\n",
    "anxiety 81293\n",
    "autism 28292\n",
    "bipolar 61506\n",
    "conversation 2118284\n",
    "depression 179615\n",
    "happy 50533\n",
    "jokes 538659\n",
    "schizophrenia 14137\n",
    "selfharm 73660\n",
    "\n",
    "Total = 3217764"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "group donates 90 defibrillators to indiana state police\n",
      "['addiction', 'anxiety', 'autism', 'bipolar', 'conversation', 'depression', 'happy', 'jokes', 'schizophrenia', 'selfharm']\n",
      "2285377\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nmini = 16\\nmaxi = 32\\n\\ntraining_dict = min_max(training_dict, mini,maxi)\\nvalidation_dict = min_max(validation_dict, mini,maxi)\\ntest_dict = min_max(test_dict, mini,maxi)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dict, validation_dict, test_dict, all_data, all_categories  \\\n",
    "= pickle.load( open( \"trvaltest/trvaltest_10cl_5-100words.p\", \"rb\" ) )\n",
    "#= pickle.load( open( \"trvaltest/emotrvaltest6.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "print(len(validation_dict['autism']))\n",
    "print(validation_dict['happy'][4])\n",
    "#all_categories = all_categories2\n",
    "print(all_categories)\n",
    "\n",
    "def min_max(data_dict, mini, maxi):\n",
    "    for key in data_dict.keys():\n",
    "        data_dict[key] = [line for line in data_dict[key] if len(line.split(' ')) in range(mini,maxi)]\n",
    "        print(key,len(data_dict[key]))\n",
    "    return data_dict\n",
    "\n",
    "def count_samples(data_dict):\n",
    "    count = 0\n",
    "    for key in data_dict.keys():\n",
    "        count += len(data_dict[key])\n",
    "    print(count)\n",
    "    return count\n",
    "\n",
    "count = count_samples(training_dict)\n",
    "'''\n",
    "mini = 16\n",
    "maxi = 32\n",
    "\n",
    "training_dict = min_max(training_dict, mini,maxi)\n",
    "validation_dict = min_max(validation_dict, mini,maxi)\n",
    "test_dict = min_max(test_dict, mini,maxi)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_dict = {}\n",
    "new_dict[\"happy\"] = [\n",
    "\" i have discovered my life's purpose  \" ,\n",
    "\" i am no longer sad , i never was worthless, i wasnt alone, i havent been wasting my life, I will make a better world \",\n",
    "\" i am not sad , i not worthless, i am not alone, I can change the world \" ,\n",
    "\" i have friends \",\n",
    "\" i am not alone \" ,\n",
    "\" the future looks bright \",\n",
    "\" i care about other people and our future \" ,\n",
    "\" my optimism is regaining it's strength \" ,\n",
    "\" there is a big world out there full of different people and my most interesting days have yet to be experienced\" ,\n",
    "\" i am more optimistic nowdays \",\n",
    "\" i have been feeling more optimistic recently\",\n",
    "\" tomorrow will be better \",\n",
    "\" all the darkness, hopelessness, insecurity, abuse, shame, and loneliness, was all part of a beautiful drama that shaped and molded me into who I am today and allows me to empathize with the pain of others. \\\n",
    "Without it I would never have realized my true calling in life, to connect, heal and unite people \",\n",
    "\" love, success, friendship, will be mine \", \n",
    "\" love, success, friendship, was meant for me\"\n",
    "]\n",
    "\n",
    "new_dict[\"depression\"] = [\n",
    "\"  i have no friends\"  ,\n",
    "\" i am alone \",\n",
    "\" happiness was not meant for me\" ,\n",
    "\" love, success, friendship, will never be mine \", \n",
    "\" love, success, friendship, just wasnt meant for me\"\n",
    "\n",
    "]\n",
    "\n",
    "validation_dict[\"happy\"].extend(new_dict[\"happy\"])\n",
    "validation_dict[\"depression\"].extend(new_dict[\"depression\"])\n",
    "\n",
    "for i in range(2):\n",
    "    training_dict[\"happy\"].extend(new_dict[\"happy\"])\n",
    "    training_dict[\"depression\"].extend(new_dict[\"depression\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## index2word, word2index, embedding\n",
    "Load the word embedding and the dictionaries for converting back and forth between the word as a string and it's index, the index2word has 4 more indices in it than the word2index because index2word has these extra tokens:\n",
    "\n",
    "1. padding extra space, PAD, indexed at 0\n",
    "2. start of sentence, SOS, indexed at 1\n",
    "3. end of sentence, EOS, indexed at 2\n",
    "4. unknown token, UNK, indexed at 3\n",
    "\n",
    "To understand how the Vocabulary() class generates these embeddings and determine our vocabulary, see embedding.ipynb, to make a new set of index2word, word2index and embedding run this (requires torchtext):\n",
    "\n",
    "`\n",
    "glove = vocab.GloVe(name='6B', dim=100) # available in 100\n",
    "print('Loaded {} words'.format(len(glove.itos)))\n",
    "vocabClass = Vocabulary()\n",
    "min_word_count = 5\n",
    "path_to_folder_of_text = 'subreddit_posts'\n",
    "index2word, word2index, embedding = vocabClass.makeEmbedding(min_word_count, glove, path_to_folder_of_text)\n",
    "dicts_embed = (index2word, word2index, embedding)\n",
    "pickle.dump(dicts_embed, open(\"embeddings/dicts_embed.p\", \"wb\" ) )\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index2word, word2index, embedding  = pickle.load( open( \"embeddings/dicts_embed_min40_folder4.p\", \"rb\" ) )\n",
    "vocabClass = Vocabulary()\n",
    "vocabClass.index2word = index2word\n",
    "vocabClass.word2index = word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9754"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabClass.word2index[\"youre\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions that support Training\n",
    "\n",
    "Before we get started training lets go over some of the functions we will import to assist with the training process. If you look below at a few of the training / validation sentences you can see that in the raw data we have not yet made all the words lowercase and we have not added a space between the last word and the ending punctuation, ie 'Home?' should be 'home ?' so that 'home' and '?' are recognized as 2 separate tokens rather than the UNK token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_categories = 10\n",
      "example of category:  happy\n",
      "giving 100 roses to strangers ! ! valentines day in hawaii\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19479"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_categories = len(all_categories)\n",
    "print('n_categories =', n_categories)\n",
    "category = random.choice(all_categories)\n",
    "print('example of category: ',category)\n",
    "print(validation_dict[category][0])\n",
    "vocabClass.word2index['thats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        #self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "                       nn.Linear(hidden_size*2, hidden_size),\n",
    "                       #nn.BatchNorm1d(num_features=1), # NEW\n",
    "                       nn.PReLU(),\n",
    "                       nn.Linear(hidden_size, 1)\n",
    "                    )\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        seq_len = encoder_outputs.size(0)\n",
    "        this_batch_size = encoder_outputs.size(1)\n",
    "        # print(' hidden.size(), encoder_outputs.size()', hidden.size(), encoder_outputs.size()) \n",
    "        # torch.Size([batch_size, hidden_size]) torch.Size([seq_len, batch_size, hidden_size])\n",
    "        # Create variable to store attention energies\n",
    "        attn_energies = Variable(torch.zeros(this_batch_size, seq_len)) # B x S\n",
    "\n",
    "        if use_cuda:\n",
    "            attn_energies = attn_energies.cuda()\n",
    "\n",
    "        # For each batch of encoder outputs\n",
    "        for b in range(this_batch_size):\n",
    "            # Calculate energy for each encoder output\n",
    "            for i in range(seq_len):\n",
    "                #attn_energies[b, i] = self.score(hidden[:, b], encoder_outputs[i, b].unsqueeze(0))\n",
    "                attn_energies[b, i] = self.score(hidden[b], encoder_outputs[i, b])\n",
    "\n",
    "        attn_weights = F.softmax(attn_energies,dim=1).unsqueeze(1) # batch_size,1,seq_len\n",
    "        \n",
    "        return attn_weights \n",
    "    \n",
    "    def score(self, hidden, encoder_output):\n",
    "        #print(hidden.size(), encoder_output.size())\n",
    "        #concat = torch.cat((hidden, encoder_output), 1)\n",
    "        concat = torch.cat((hidden, encoder_output))\n",
    "        energy = self.fc1(concat)\n",
    "        return energy\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, embedding, output_size, num_layers = 3, bidirectional = False, \n",
    "                 train_embedding = True , dropout = 0.0):\n",
    "        \n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.dropout = nn.Dropout(p=dropout) # p – probability of an element to be zeroed. Default: 0.5\n",
    "        \n",
    "        embedding = torch.from_numpy(embedding).float()\n",
    "        \n",
    "        if use_cuda:\n",
    "            embedding.cuda()\n",
    "        \n",
    "        self.embedding = nn.Embedding(embedding.shape[0], embedding.shape[1])\n",
    "        self.embedding.weight = nn.Parameter(embedding, requires_grad=train_embedding)\n",
    "        self.gru = nn.GRU(embedding.shape[1], hidden_size, num_layers, \n",
    "                          bidirectional=bidirectional, dropout = dropout)\n",
    "        \n",
    "        if bidirectional:\n",
    "            num_directions = 2\n",
    "        else:\n",
    "            num_directions = 1\n",
    "        \n",
    "        # make the initial hidden state learnable as well \n",
    "        hidden0 = torch.zeros(self.num_layers*num_directions, 1, self.hidden_size)\n",
    "        self.hidden0 = nn.Parameter(hidden0, requires_grad=True)\n",
    "        \n",
    "        self.num_cells = num_layers*num_directions\n",
    "        \n",
    "        self.fc_concat = nn.Sequential(\n",
    "                             nn.Linear(hidden_size * 2, hidden_size),\n",
    "                             nn.BatchNorm1d(num_features=self.hidden_size)\n",
    "                             ) \n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "                       nn.Linear(self.hidden_size,self.hidden_size),\n",
    "                       nn.BatchNorm1d(num_features=self.hidden_size),\n",
    "                       )\n",
    "        \n",
    "        self.out = nn.Linear(hidden_size,output_size)\n",
    "        self.prelu = nn.PReLU()\n",
    "        self.attn = Attn(hidden_size)\n",
    "\n",
    "    def forward(self, input_seqs):\n",
    "        \n",
    "        batch_size = input_seqs.size(1)\n",
    "        hidden = self.hidden0.repeat(1, batch_size, 1)\n",
    "        self.embedded = self.embedding(input_seqs)\n",
    "        encoder_outputs, last_seq_hidden = self.gru(self.embedded, hidden)\n",
    "        last_hidden = last_seq_hidden[-1] \n",
    "        \n",
    "        attn_weights = self.attn(last_hidden, encoder_outputs) #  batch_size,1,seq_len\n",
    "        \n",
    "        # output of GRU (seq_len, batch_size, hidden_size) -> (batch_size, seq_len, hidden_size)\n",
    "        encoder_outputs_bsh = encoder_outputs.transpose(0, 1)\n",
    "        \n",
    "        # bmm does operation (b,1,s).bmm(b,s,h) = (b,1,h)\n",
    "        context = torch.bmm(attn_weights,encoder_outputs_bsh) # should be  # B x S=1 x N\n",
    "        \n",
    "        # Attentional vector using the RNN hidden state and context vector concatenated together \n",
    "        context = context.squeeze(1)       # B x S=1 x H -> Batch Size x Hidden Size\n",
    "        concat_input = torch.cat((last_hidden, context), 1) # both should be batch_size x hidden_size\n",
    "        \n",
    "        concat_output = F.tanh(self.fc_concat(concat_input)) # <hidden_size>\n",
    "\n",
    "        fc1 = self.fc1(concat_output)\n",
    "\n",
    "        output = self.out(self.dropout(self.prelu(fc1)))\n",
    "        \n",
    "        return output, fc1 , attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trained models\n",
    "\n",
    "## v5 naming convention: `attn_v5_(number of hidden units)_(number of layers)`\n",
    "\n",
    "\n",
    "## Once the accuracy is plateau'd on a learning rate decay from 0.01-0.001 and batch size 32 - 64 , good for fine tuning is a learning rate of 0.00025 - 0.0001 with batch size 2 - 8 \n",
    "\n",
    "For example:\n",
    "\n",
    "batch_size = 8, \n",
    "learning_rate = 0.00025,\n",
    "gamma = 0.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hidden = 1024\n",
    "num_layers = 4\n",
    "bidirectional = False\n",
    "\n",
    "rnn = RNN( n_hidden, embedding, n_categories, num_layers = num_layers, \n",
    "          bidirectional = bidirectional, dropout = 0.3)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if use_cuda:\n",
    "    rnn = rnn.cuda()\n",
    "\n",
    "name = 'trvaltest_10cl_5-100w_embed_min40_v0_'+str(n_hidden)+'_'+str(num_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_dict(vocabClass,data_dict):\n",
    "    rnn.train(False)\n",
    "    count = 0\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    \n",
    "    for category in data_dict.keys():\n",
    "        category_tensor = Variable(torch.LongTensor([all_categories.index(category)]))\n",
    "        if use_cuda:\n",
    "            category_tensor = category_tensor.cuda()\n",
    "        for line in data_dict[category]:\n",
    "            line_tensor = Variable(torch.LongTensor(indexesFromSentence(vocabClass, line))).view(-1,1)\n",
    "            #print(line_tensor.size())\n",
    "            if use_cuda:\n",
    "                line_tensor = line_tensor.cuda()\n",
    "            output, vector_rep, attn_wts = rnn(line_tensor)\n",
    "            loss = criterion(output, category_tensor)\n",
    "            total_loss += loss\n",
    "            count += 1\n",
    "            topv, topi = output.data.topk(1, 1, True)\n",
    "            category_index = topi[0][0]\n",
    "            if category == all_categories[category_index]:\n",
    "                total_correct += 1\n",
    "                \n",
    "    avg_loss = float(total_loss)/count\n",
    "    accuracy = float(total_correct)/count\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def random_training_batch(batch_size,vocabClass):\n",
    "    \n",
    "    category_tensor = torch.LongTensor(batch_size).random_(1)\n",
    "    input_seqs = []\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        category = random.choice(all_categories)\n",
    "        category_tensor[b] = all_categories.index(category)\n",
    "        line = random.choice(training_dict[category])\n",
    "        input_seqs.append(indexesFromSentence(vocabClass, line))\n",
    "        \n",
    "    input_lengths = [len(s) for s in input_seqs]\n",
    "    \n",
    "    input_padded = [pad_seq(s, max(input_lengths), vocabClass) for s in input_seqs]\n",
    "        \n",
    "    category_tensor = Variable(category_tensor)\n",
    "    input_var = Variable(torch.LongTensor(input_padded)).transpose(0, 1)\n",
    "    # print(input_var.size()) <max_length, batch_size>\n",
    "    if use_cuda:\n",
    "        input_var = input_var.cuda()\n",
    "        category_tensor = category_tensor.cuda()\n",
    "        \n",
    "    return category_tensor, input_var, input_lengths\n",
    "\n",
    "def save_model(rnn,name):\n",
    "\n",
    "    torch.save(rnn.state_dict(), \"modelstate/\"+name+\".pth\")\n",
    "    rnncpu = rnn.cpu()\n",
    "    torch.save(rnncpu.state_dict(), \"modelstate/\"+name+\"_cpu.pth\")\n",
    "    rnn.cuda()\n",
    "\n",
    "# https://pytorch.org/docs/stable/_modules/torch/nn/utils/clip_grad.html\n",
    "    \n",
    "def train(category_tensor, line_tensor, rnn, max_norm, norm_type):\n",
    "    rnn.zero_grad()\n",
    "    rnn.train(True)\n",
    "    output, vector_rep, attn_wts = rnn(line_tensor)\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "    #total_norm = max(p.grad.data.abs().max() for p in rnn.parameters())\n",
    "    #print(total_norm.cpu().numpy())\n",
    "    param_norm = torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm , norm_type=norm_type)\n",
    "    #total_norm = max(p.grad.data.abs().max() for p in rnn.parameters())\n",
    "    #print(total_norm.cpu().numpy())\n",
    "    optimizer.step()\n",
    "    return output, loss.item(), param_norm.cpu().numpy() #loss.data[0], param_norm\n",
    "\n",
    "def train2(category_tensor, line_tensor, rnn, clip = 0.1):\n",
    "    rnn.zero_grad()\n",
    "    rnn.train(True)\n",
    "    output, vector_rep, attn_wts = rnn(line_tensor)\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "    #param_norm = torch.nn.utils.clip_grad_norm_(rnn.parameters(), 10 , norm_type=2)\n",
    "    #print(\"param norm: \", param_norm.cpu().numpy())\n",
    "    for p in rnn.parameters():\n",
    "        p.grad.data.clamp_(max=clip)\n",
    "    #param_norm = torch.nn.utils.clip_grad_norm_(rnn.parameters(), 10 , norm_type=2)\n",
    "    #print(\"param norm: \", param_norm.cpu().numpy())\n",
    "    optimizer.step()\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_val_loss 1.8824 ,  val_accuracy 0.3971\n"
     ]
    }
   ],
   "source": [
    "name = 'trvaltest_10cl_5-100w_embed_min40_v2_'+str(n_hidden)+'_'+str(num_layers)\n",
    "\n",
    "rnn.load_state_dict(torch.load(\"modelstate/\"+name+\".pth\"))\n",
    "avg_val_loss, val_accuracy = eval_dict(vocabClass,validation_dict)\n",
    "print('avg_val_loss %.4f ,  val_accuracy %.4f' % (avg_val_loss, val_accuracy)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last LR  0.000077\n",
      "estimates time  91.02222222222223  hours\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "n_epochs = 102400*5\n",
    "plot_every = 512\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "gamma = 0.95\n",
    "step_size=n_epochs/50\n",
    "L2_reg = 1e-5\n",
    "clip = 1.0\n",
    "norm_type = 2 # \"inf\"\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate, weight_decay=L2_reg)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "start = time.time()\n",
    "\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "current_vloss = 0\n",
    "all_vlosses = []\n",
    "\n",
    "#name = 'tvt10_em4_'+str(n_hidden)+'_'+str(num_layers) \n",
    "#rnn.load_state_dict(torch.load(\"modelstate/\"+name+\".pth\"))\n",
    "best_val_accuracy =  val_accuracy\n",
    "#avg_val_loss, best_val_accuracy = eval_validation(vocabClass)\n",
    "#print('avg_val_loss %.4f ,  val_accuracy %.4f' % (avg_val_loss, best_val_accuracy))\n",
    "print('last LR  %.6f' % (gamma**(n_epochs/step_size)*learning_rate))\n",
    "print('estimates time ', (batch_size*n_epochs)/(3000*60), ' hours')\n",
    "#param_norm = torch.nn.utils.clip_grad_norm_(rnn.parameters(), clip)\n",
    "#print(\"param norm: \", param_norm)\n",
    "\n",
    "#total_norm = max(p.grad.data.abs().max() for p in rnn.parameters())\n",
    "#print(total_norm)\n",
    "\n",
    "name = 'trvaltest_10cl_5-100w_embed_min40_v2_'+str(n_hidden)+'_'+str(num_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#category_tensor, input_var, input_lengths = random_training_batch(batch_size,vocabClass)\n",
    "#output, loss, param_norm = train(category_tensor, input_var, rnn, max_norm = 0.1, norm_type = \"inf\")\n",
    "#print(\"param norm: \", param_norm.cpu().numpy())\n",
    "#avg_val_loss, val_accuracy = eval_dict(vocabClass,validation_dict)\n",
    "#print('avg_val_loss %.4f ,  val_accuracy %.4f' % (avg_val_loss, val_accuracy)) \n",
    "#rnn.zero_grad()\n",
    "#param_norm = torch.nn.utils.clip_grad_norm_(rnn.parameters(), clip)\n",
    "#print(\"param norm: \", param_norm.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "training take a long time and progress at the end is more sparse, to give you an idea, a 256 hidden unit, 2 layer RNN with the attention FFNN's at the end training for 32000 batches at 8 samples a batch takes 199 minutes. so 256000 samples takes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 512, prm nrm: 1.7626, val acc: 0.3431, validation loss: 1.8265, training loss: 1.5519  0% (16m 8s)\n",
      "saving model...\n",
      "avg_val_loss 1.8226 ,  val_accuracy 0.4108\n",
      "epoch 1024, prm nrm: 1.5732, val acc: 0.4108, validation loss: 1.8226, training loss: 1.4656  0% (32m 3s)\n",
      "saving model...\n",
      "avg_val_loss 1.6844 ,  val_accuracy 0.4412\n",
      "epoch 1536, prm nrm: 1.7469, val acc: 0.4412, validation loss: 1.6844, training loss: 1.4290  0% (48m 4s)\n",
      "saving model...\n",
      "avg_val_loss 1.5282 ,  val_accuracy 0.4814\n",
      "epoch 2048, prm nrm: 1.2894, val acc: 0.4814, validation loss: 1.5282, training loss: 1.3870  0% (64m 8s)\n",
      "saving model...\n",
      "avg_val_loss 1.5330 ,  val_accuracy 0.5108\n",
      "epoch 2560, prm nrm: 1.3940, val acc: 0.5108, validation loss: 1.5330, training loss: 1.3571  0% (80m 7s)\n",
      "epoch 3072, prm nrm: 1.4404, val acc: 0.4608, validation loss: 1.7492, training loss: 1.3148  0% (96m 4s)\n",
      "epoch 3584, prm nrm: 1.3661, val acc: 0.4765, validation loss: 1.5633, training loss: 1.2992  0% (112m 6s)\n",
      "epoch 4096, prm nrm: 0.9619, val acc: 0.4412, validation loss: 1.6595, training loss: 1.2882  0% (128m 9s)\n",
      "epoch 4608, prm nrm: 1.1619, val acc: 0.4775, validation loss: 1.5206, training loss: 1.2800  0% (144m 8s)\n",
      "saving model...\n",
      "avg_val_loss 1.4913 ,  val_accuracy 0.5186\n",
      "epoch 5120, prm nrm: 1.4026, val acc: 0.5186, validation loss: 1.4913, training loss: 1.2735  1% (160m 10s)\n",
      "epoch 5632, prm nrm: 0.9941, val acc: 0.5108, validation loss: 1.4814, training loss: 1.2669  1% (176m 14s)\n",
      "epoch 6144, prm nrm: 1.0804, val acc: 0.5039, validation loss: 1.5560, training loss: 1.2520  1% (192m 16s)\n",
      "saving model...\n",
      "avg_val_loss 1.5071 ,  val_accuracy 0.5216\n",
      "epoch 6656, prm nrm: 1.5294, val acc: 0.5216, validation loss: 1.5071, training loss: 1.2332  1% (208m 10s)\n",
      "saving model...\n",
      "avg_val_loss 1.4422 ,  val_accuracy 0.5471\n",
      "epoch 7168, prm nrm: 1.5706, val acc: 0.5471, validation loss: 1.4422, training loss: 1.1949  1% (224m 2s)\n",
      "epoch 7680, prm nrm: 1.1769, val acc: 0.5324, validation loss: 1.5511, training loss: 1.1962  1% (239m 58s)\n",
      "epoch 8192, prm nrm: 1.0068, val acc: 0.5284, validation loss: 1.4801, training loss: 1.2015  1% (255m 46s)\n",
      "epoch 8704, prm nrm: 1.1362, val acc: 0.5373, validation loss: 1.5814, training loss: 1.1852  1% (271m 43s)\n",
      "saving model...\n",
      "avg_val_loss 1.4762 ,  val_accuracy 0.5490\n",
      "epoch 9216, prm nrm: 1.3872, val acc: 0.5490, validation loss: 1.4762, training loss: 1.1690  1% (287m 41s)\n",
      "saving model...\n",
      "avg_val_loss 1.4088 ,  val_accuracy 0.5510\n",
      "epoch 9728, prm nrm: 1.0994, val acc: 0.5510, validation loss: 1.4088, training loss: 1.1681  1% (303m 59s)\n",
      "saving model...\n",
      "avg_val_loss 1.3650 ,  val_accuracy 0.5529\n",
      "epoch 10240, prm nrm: 1.1804, val acc: 0.5529, validation loss: 1.3650, training loss: 1.1585  2% (319m 57s)\n",
      "epoch 10752, prm nrm: 0.9522, val acc: 0.5441, validation loss: 1.4978, training loss: 1.1310  2% (336m 10s)\n",
      "saving model...\n",
      "avg_val_loss 1.3803 ,  val_accuracy 0.5706\n",
      "epoch 11264, prm nrm: 0.8377, val acc: 0.5706, validation loss: 1.3803, training loss: 1.1394  2% (352m 19s)\n",
      "epoch 11776, prm nrm: 0.9660, val acc: 0.5608, validation loss: 1.4358, training loss: 1.1299  2% (368m 8s)\n",
      "saving model...\n",
      "avg_val_loss 1.3726 ,  val_accuracy 0.5745\n",
      "epoch 12288, prm nrm: 1.2068, val acc: 0.5745, validation loss: 1.3726, training loss: 1.1240  2% (384m 1s)\n",
      "saving model...\n",
      "avg_val_loss 1.4206 ,  val_accuracy 0.5775\n",
      "epoch 12800, prm nrm: 1.3805, val acc: 0.5775, validation loss: 1.4206, training loss: 1.1099  2% (400m 8s)\n",
      "epoch 13312, prm nrm: 0.9044, val acc: 0.5608, validation loss: 1.3904, training loss: 1.1107  2% (415m 57s)\n",
      "epoch 13824, prm nrm: 0.9304, val acc: 0.5392, validation loss: 1.4632, training loss: 1.0956  2% (431m 48s)\n",
      "epoch 14336, prm nrm: 1.0446, val acc: 0.5588, validation loss: 1.5374, training loss: 1.0800  2% (447m 52s)\n",
      "epoch 14848, prm nrm: 0.9419, val acc: 0.5529, validation loss: 1.4521, training loss: 1.0849  2% (464m 5s)\n",
      "epoch 15360, prm nrm: 1.4316, val acc: 0.5578, validation loss: 1.4403, training loss: 1.0943  3% (480m 10s)\n",
      "epoch 15872, prm nrm: 2.6885, val acc: 0.5520, validation loss: 1.4088, training loss: 1.0790  3% (496m 12s)\n",
      "epoch 16384, prm nrm: 1.0052, val acc: 0.5608, validation loss: 1.3913, training loss: 1.0650  3% (512m 19s)\n",
      "epoch 16896, prm nrm: 1.0819, val acc: 0.5539, validation loss: 1.3516, training loss: 1.0584  3% (528m 8s)\n",
      "epoch 17408, prm nrm: 1.2479, val acc: 0.5490, validation loss: 1.4302, training loss: 1.0515  3% (544m 11s)\n",
      "epoch 17920, prm nrm: 1.1101, val acc: 0.5539, validation loss: 1.5216, training loss: 1.0582  3% (560m 29s)\n",
      "epoch 18432, prm nrm: 1.1351, val acc: 0.5608, validation loss: 1.4590, training loss: 1.0618  3% (576m 36s)\n",
      "epoch 18944, prm nrm: 0.9588, val acc: 0.5618, validation loss: 1.3826, training loss: 1.0490  3% (592m 35s)\n",
      "epoch 19456, prm nrm: 1.1089, val acc: 0.5598, validation loss: 1.4268, training loss: 1.0446  3% (608m 54s)\n",
      "epoch 19968, prm nrm: 1.7473, val acc: 0.5725, validation loss: 1.4712, training loss: 1.0334  3% (625m 5s)\n",
      "epoch 20480, prm nrm: 1.1844, val acc: 0.5716, validation loss: 1.3864, training loss: 1.0327  4% (641m 6s)\n",
      "epoch 20992, prm nrm: 1.0638, val acc: 0.5716, validation loss: 1.3920, training loss: 1.0152  4% (657m 13s)\n",
      "epoch 21504, prm nrm: 1.0035, val acc: 0.5412, validation loss: 1.4383, training loss: 1.0211  4% (673m 34s)\n",
      "epoch 22016, prm nrm: 0.8967, val acc: 0.5627, validation loss: 1.4359, training loss: 0.9994  4% (689m 47s)\n",
      "epoch 22528, prm nrm: 0.9616, val acc: 0.5745, validation loss: 1.4919, training loss: 0.9910  4% (705m 57s)\n",
      "epoch 23040, prm nrm: 1.0695, val acc: 0.5725, validation loss: 1.4427, training loss: 1.0209  4% (721m 54s)\n",
      "epoch 23552, prm nrm: 1.0075, val acc: 0.5559, validation loss: 1.4349, training loss: 0.9997  4% (737m 49s)\n",
      "epoch 24064, prm nrm: 1.0035, val acc: 0.5745, validation loss: 1.4601, training loss: 1.0024  4% (753m 58s)\n",
      "epoch 24576, prm nrm: 1.1520, val acc: 0.5500, validation loss: 1.5415, training loss: 0.9792  4% (770m 9s)\n",
      "epoch 25088, prm nrm: 1.0368, val acc: 0.5706, validation loss: 1.4439, training loss: 0.9785  4% (786m 12s)\n",
      "epoch 25600, prm nrm: 1.0180, val acc: 0.5725, validation loss: 1.4048, training loss: 0.9835  5% (802m 17s)\n",
      "epoch 26112, prm nrm: 1.0789, val acc: 0.5569, validation loss: 1.4211, training loss: 0.9748  5% (818m 31s)\n",
      "saving model...\n",
      "avg_val_loss 1.4379 ,  val_accuracy 0.5804\n",
      "epoch 26624, prm nrm: 1.0945, val acc: 0.5804, validation loss: 1.4379, training loss: 0.9898  5% (834m 50s)\n",
      "epoch 27136, prm nrm: 0.9717, val acc: 0.5706, validation loss: 1.4165, training loss: 0.9693  5% (850m 53s)\n",
      "epoch 27648, prm nrm: 1.2579, val acc: 0.5794, validation loss: 1.4669, training loss: 0.9693  5% (866m 59s)\n",
      "saving model...\n",
      "avg_val_loss 1.4673 ,  val_accuracy 0.5814\n",
      "epoch 28160, prm nrm: 1.3716, val acc: 0.5814, validation loss: 1.4673, training loss: 0.9569  5% (883m 3s)\n",
      "epoch 28672, prm nrm: 0.8697, val acc: 0.5775, validation loss: 1.4295, training loss: 0.9559  5% (899m 8s)\n",
      "epoch 29184, prm nrm: 1.0749, val acc: 0.5480, validation loss: 1.4174, training loss: 0.9546  5% (915m 11s)\n",
      "epoch 29696, prm nrm: 0.9902, val acc: 0.5775, validation loss: 1.4109, training loss: 0.9500  5% (931m 13s)\n",
      "epoch 30208, prm nrm: 1.3063, val acc: 0.5618, validation loss: 1.4959, training loss: 0.9381  5% (947m 26s)\n",
      "epoch 30720, prm nrm: 1.1305, val acc: 0.5794, validation loss: 1.4442, training loss: 0.9497  6% (963m 38s)\n",
      "epoch 31232, prm nrm: 1.0211, val acc: 0.5706, validation loss: 1.4375, training loss: 0.9307  6% (979m 55s)\n",
      "epoch 31744, prm nrm: 0.8435, val acc: 0.5549, validation loss: 1.5144, training loss: 0.9375  6% (996m 0s)\n",
      "epoch 32256, prm nrm: 1.0793, val acc: 0.5745, validation loss: 1.4267, training loss: 0.9204  6% (1012m 21s)\n",
      "epoch 32768, prm nrm: 0.8905, val acc: 0.5578, validation loss: 1.5353, training loss: 0.9144  6% (1028m 29s)\n",
      "epoch 33280, prm nrm: 1.1985, val acc: 0.5745, validation loss: 1.4360, training loss: 0.9201  6% (1044m 52s)\n",
      "epoch 33792, prm nrm: 1.1371, val acc: 0.5706, validation loss: 1.4622, training loss: 0.9152  6% (1061m 1s)\n",
      "epoch 34304, prm nrm: 1.0079, val acc: 0.5627, validation loss: 1.4477, training loss: 0.9109  6% (1077m 12s)\n",
      "epoch 34816, prm nrm: 1.4442, val acc: 0.5608, validation loss: 1.6746, training loss: 0.9024  6% (1093m 0s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 35328, prm nrm: 1.1059, val acc: 0.5510, validation loss: 1.5831, training loss: 0.8988  6% (1109m 2s)\n",
      "epoch 35840, prm nrm: 1.1041, val acc: 0.5716, validation loss: 1.5215, training loss: 0.9148  7% (1125m 7s)\n",
      "epoch 36352, prm nrm: 1.8349, val acc: 0.5431, validation loss: 1.5025, training loss: 0.8907  7% (1141m 8s)\n",
      "epoch 36864, prm nrm: 3.4034, val acc: 0.5784, validation loss: 1.5962, training loss: 0.9013  7% (1157m 21s)\n",
      "epoch 37376, prm nrm: 0.9880, val acc: 0.5755, validation loss: 1.4988, training loss: 0.8961  7% (1173m 19s)\n",
      "epoch 37888, prm nrm: 1.0127, val acc: 0.5735, validation loss: 1.4967, training loss: 0.8957  7% (1189m 23s)\n",
      "saving model...\n",
      "avg_val_loss 1.4756 ,  val_accuracy 0.5833\n",
      "epoch 38400, prm nrm: 0.9951, val acc: 0.5833, validation loss: 1.4756, training loss: 0.9017  7% (1205m 27s)\n",
      "epoch 38912, prm nrm: 1.4103, val acc: 0.5735, validation loss: 1.5488, training loss: 0.8910  7% (1221m 44s)\n",
      "epoch 39424, prm nrm: 1.3030, val acc: 0.5745, validation loss: 1.5991, training loss: 0.8780  7% (1237m 50s)\n",
      "epoch 39936, prm nrm: 1.3817, val acc: 0.5824, validation loss: 1.4488, training loss: 0.8903  7% (1254m 4s)\n",
      "epoch 40448, prm nrm: 0.8748, val acc: 0.5725, validation loss: 1.5340, training loss: 0.8771  7% (1270m 17s)\n",
      "epoch 40960, prm nrm: 1.8058, val acc: 0.5745, validation loss: 1.5628, training loss: 0.8803  8% (1286m 27s)\n",
      "epoch 41472, prm nrm: 1.3359, val acc: 0.5696, validation loss: 1.5571, training loss: 0.8639  8% (1302m 36s)\n",
      "saving model...\n",
      "avg_val_loss 1.4919 ,  val_accuracy 0.5912\n",
      "epoch 41984, prm nrm: 2.7168, val acc: 0.5912, validation loss: 1.4919, training loss: 0.8586  8% (1318m 54s)\n",
      "epoch 42496, prm nrm: 0.7953, val acc: 0.5676, validation loss: 1.5147, training loss: 0.8627  8% (1335m 3s)\n",
      "epoch 43008, prm nrm: 1.2000, val acc: 0.5735, validation loss: 1.5006, training loss: 0.8438  8% (1351m 3s)\n",
      "epoch 43520, prm nrm: 0.8748, val acc: 0.5755, validation loss: 1.5360, training loss: 0.8508  8% (1367m 9s)\n",
      "epoch 44032, prm nrm: 1.6614, val acc: 0.5676, validation loss: 1.5522, training loss: 0.8496  8% (1383m 15s)\n",
      "epoch 44544, prm nrm: 1.1153, val acc: 0.5686, validation loss: 1.5659, training loss: 0.8496  8% (1399m 29s)\n",
      "epoch 45056, prm nrm: 1.2570, val acc: 0.5716, validation loss: 1.5625, training loss: 0.8526  8% (1415m 46s)\n",
      "epoch 45568, prm nrm: 1.3182, val acc: 0.5647, validation loss: 1.5793, training loss: 0.8467  8% (1431m 54s)\n",
      "epoch 46080, prm nrm: 1.2356, val acc: 0.5833, validation loss: 1.5342, training loss: 0.8647  9% (1447m 55s)\n",
      "epoch 46592, prm nrm: 1.3019, val acc: 0.5843, validation loss: 1.5989, training loss: 0.8460  9% (1464m 4s)\n",
      "epoch 47104, prm nrm: 1.2175, val acc: 0.5824, validation loss: 1.4926, training loss: 0.8401  9% (1479m 58s)\n",
      "saving model...\n",
      "avg_val_loss 1.5329 ,  val_accuracy 0.5922\n",
      "epoch 47616, prm nrm: 1.2695, val acc: 0.5922, validation loss: 1.5329, training loss: 0.8245  9% (1496m 6s)\n",
      "saving model...\n",
      "avg_val_loss 1.5158 ,  val_accuracy 0.6000\n",
      "epoch 48128, prm nrm: 1.2076, val acc: 0.6000, validation loss: 1.5158, training loss: 0.8301  9% (1512m 9s)\n",
      "epoch 48640, prm nrm: 0.9287, val acc: 0.5784, validation loss: 1.5522, training loss: 0.8296  9% (1528m 20s)\n",
      "epoch 49152, prm nrm: 1.6263, val acc: 0.5745, validation loss: 1.5188, training loss: 0.8362  9% (1544m 24s)\n",
      "epoch 49664, prm nrm: 1.4585, val acc: 0.5755, validation loss: 1.4957, training loss: 0.8437  9% (1560m 21s)\n",
      "epoch 50176, prm nrm: 1.0552, val acc: 0.5745, validation loss: 1.6138, training loss: 0.8260  9% (1576m 40s)\n",
      "epoch 50688, prm nrm: 1.3786, val acc: 0.5755, validation loss: 1.5651, training loss: 0.8339  9% (1592m 55s)\n",
      "epoch 51200, prm nrm: 1.2981, val acc: 0.5931, validation loss: 1.6307, training loss: 0.8193  10% (1609m 17s)\n",
      "epoch 51712, prm nrm: 2.1260, val acc: 0.5853, validation loss: 1.5302, training loss: 0.8185  10% (1625m 21s)\n",
      "epoch 52224, prm nrm: 1.0069, val acc: 0.5706, validation loss: 1.5102, training loss: 0.8132  10% (1641m 33s)\n",
      "epoch 52736, prm nrm: 1.2042, val acc: 0.5657, validation loss: 1.4642, training loss: 0.8350  10% (1657m 48s)\n",
      "epoch 53248, prm nrm: 1.0118, val acc: 0.5627, validation loss: 1.5719, training loss: 0.7874  10% (1674m 17s)\n",
      "epoch 53760, prm nrm: 2.0921, val acc: 0.5745, validation loss: 1.6084, training loss: 0.8122  10% (1690m 32s)\n",
      "epoch 54272, prm nrm: 1.2682, val acc: 0.5588, validation loss: 1.5434, training loss: 0.8069  10% (1706m 31s)\n",
      "epoch 54784, prm nrm: 1.0772, val acc: 0.5814, validation loss: 1.6102, training loss: 0.8142  10% (1722m 39s)\n",
      "epoch 55296, prm nrm: 1.5400, val acc: 0.5549, validation loss: 1.5890, training loss: 0.7980  10% (1738m 48s)\n",
      "epoch 55808, prm nrm: 1.7821, val acc: 0.5647, validation loss: 1.5667, training loss: 0.8016  10% (1754m 46s)\n",
      "epoch 56320, prm nrm: 1.6128, val acc: 0.5706, validation loss: 1.5607, training loss: 0.7836  11% (1771m 2s)\n",
      "epoch 56832, prm nrm: 1.0824, val acc: 0.5843, validation loss: 1.6523, training loss: 0.7967  11% (1787m 4s)\n",
      "epoch 57344, prm nrm: 1.5408, val acc: 0.5706, validation loss: 1.6109, training loss: 0.7939  11% (1803m 18s)\n",
      "epoch 57856, prm nrm: 2.0531, val acc: 0.5706, validation loss: 1.5948, training loss: 0.7904  11% (1819m 2s)\n",
      "epoch 58368, prm nrm: 1.0900, val acc: 0.5588, validation loss: 1.6115, training loss: 0.7832  11% (1835m 20s)\n",
      "epoch 58880, prm nrm: 1.4389, val acc: 0.5814, validation loss: 1.5614, training loss: 0.7914  11% (1851m 22s)\n",
      "epoch 59392, prm nrm: 2.0644, val acc: 0.5745, validation loss: 1.6510, training loss: 0.7899  11% (1867m 31s)\n",
      "epoch 59904, prm nrm: 1.7564, val acc: 0.5676, validation loss: 1.5167, training loss: 0.7854  11% (1883m 36s)\n",
      "epoch 60416, prm nrm: 1.2731, val acc: 0.5716, validation loss: 1.5626, training loss: 0.7742  11% (1899m 45s)\n",
      "epoch 60928, prm nrm: 0.9459, val acc: 0.5775, validation loss: 1.5697, training loss: 0.7874  11% (1915m 52s)\n",
      "epoch 61440, prm nrm: 1.1538, val acc: 0.5824, validation loss: 1.5574, training loss: 0.7799  12% (1932m 18s)\n",
      "epoch 61952, prm nrm: 1.7355, val acc: 0.5824, validation loss: 1.5726, training loss: 0.7840  12% (1948m 22s)\n",
      "epoch 62464, prm nrm: 3.5568, val acc: 0.5627, validation loss: 1.6227, training loss: 0.7739  12% (1964m 31s)\n",
      "epoch 62976, prm nrm: 1.0565, val acc: 0.5637, validation loss: 1.6359, training loss: 0.7708  12% (1980m 24s)\n",
      "epoch 63488, prm nrm: 1.1728, val acc: 0.5804, validation loss: 1.5443, training loss: 0.7583  12% (1996m 44s)\n",
      "epoch 64000, prm nrm: 2.9386, val acc: 0.5667, validation loss: 1.6697, training loss: 0.7574  12% (2012m 54s)\n",
      "epoch 64512, prm nrm: 0.7899, val acc: 0.5745, validation loss: 1.6979, training loss: 0.7672  12% (2028m 58s)\n",
      "epoch 65024, prm nrm: 1.2353, val acc: 0.5657, validation loss: 1.5611, training loss: 0.7455  12% (2045m 11s)\n",
      "epoch 65536, prm nrm: 1.4086, val acc: 0.5520, validation loss: 1.6916, training loss: 0.7549  12% (2061m 5s)\n",
      "epoch 66048, prm nrm: 1.0310, val acc: 0.5529, validation loss: 1.6947, training loss: 0.7434  12% (2077m 23s)\n",
      "epoch 66560, prm nrm: 1.7203, val acc: 0.5627, validation loss: 1.6008, training loss: 0.7481  13% (2093m 22s)\n",
      "epoch 67072, prm nrm: 1.6037, val acc: 0.5637, validation loss: 1.6915, training loss: 0.7540  13% (2109m 37s)\n",
      "epoch 67584, prm nrm: 0.8835, val acc: 0.5794, validation loss: 1.6545, training loss: 0.7416  13% (2125m 49s)\n",
      "epoch 68096, prm nrm: 1.0877, val acc: 0.5863, validation loss: 1.5451, training loss: 0.7446  13% (2141m 54s)\n",
      "epoch 68608, prm nrm: 1.2950, val acc: 0.5755, validation loss: 1.6357, training loss: 0.7542  13% (2158m 2s)\n",
      "epoch 69120, prm nrm: 0.8000, val acc: 0.5775, validation loss: 1.6393, training loss: 0.7408  13% (2174m 17s)\n",
      "epoch 69632, prm nrm: 1.8641, val acc: 0.5824, validation loss: 1.6368, training loss: 0.7587  13% (2190m 22s)\n",
      "epoch 70144, prm nrm: 1.0566, val acc: 0.5608, validation loss: 1.6372, training loss: 0.7351  13% (2206m 30s)\n",
      "epoch 70656, prm nrm: 1.0094, val acc: 0.5804, validation loss: 1.7268, training loss: 0.7405  13% (2222m 42s)\n",
      "epoch 71168, prm nrm: 1.2286, val acc: 0.5814, validation loss: 1.6816, training loss: 0.7497  13% (2238m 37s)\n",
      "epoch 71680, prm nrm: 1.6833, val acc: 0.5775, validation loss: 1.7524, training loss: 0.7274  14% (2254m 45s)\n",
      "epoch 72192, prm nrm: 0.8712, val acc: 0.5853, validation loss: 1.6027, training loss: 0.7417  14% (2270m 45s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 72704, prm nrm: 0.9518, val acc: 0.5745, validation loss: 1.6150, training loss: 0.7257  14% (2286m 52s)\n",
      "epoch 73216, prm nrm: 1.9895, val acc: 0.5676, validation loss: 1.7029, training loss: 0.7231  14% (2302m 50s)\n",
      "epoch 73728, prm nrm: 1.0847, val acc: 0.5784, validation loss: 1.6087, training loss: 0.7427  14% (2318m 55s)\n",
      "epoch 74240, prm nrm: 0.8538, val acc: 0.5676, validation loss: 1.7160, training loss: 0.7197  14% (2335m 11s)\n",
      "epoch 74752, prm nrm: 1.2178, val acc: 0.5882, validation loss: 1.5996, training loss: 0.7228  14% (2351m 21s)\n",
      "epoch 75264, prm nrm: 1.3018, val acc: 0.5637, validation loss: 1.7704, training loss: 0.7284  14% (2367m 37s)\n",
      "epoch 75776, prm nrm: 0.9694, val acc: 0.5853, validation loss: 1.7014, training loss: 0.7254  14% (2383m 41s)\n",
      "epoch 76288, prm nrm: 1.7736, val acc: 0.5765, validation loss: 1.7046, training loss: 0.7300  14% (2399m 41s)\n",
      "epoch 76800, prm nrm: 1.1481, val acc: 0.5647, validation loss: 1.6401, training loss: 0.7144  15% (2415m 56s)\n",
      "epoch 77312, prm nrm: 2.5586, val acc: 0.5676, validation loss: 1.7774, training loss: 0.7142  15% (2432m 3s)\n",
      "epoch 77824, prm nrm: 1.8839, val acc: 0.5745, validation loss: 1.6421, training loss: 0.7120  15% (2448m 21s)\n",
      "epoch 78336, prm nrm: 1.5173, val acc: 0.5735, validation loss: 1.5563, training loss: 0.7224  15% (2464m 42s)\n",
      "epoch 78848, prm nrm: 2.2826, val acc: 0.5627, validation loss: 1.8227, training loss: 0.6953  15% (2480m 56s)\n",
      "epoch 79360, prm nrm: 1.2797, val acc: 0.5588, validation loss: 1.7099, training loss: 0.7186  15% (2497m 5s)\n",
      "epoch 79872, prm nrm: 1.4458, val acc: 0.5667, validation loss: 1.7765, training loss: 0.7120  15% (2513m 22s)\n",
      "epoch 80384, prm nrm: 2.6778, val acc: 0.5618, validation loss: 1.7152, training loss: 0.7008  15% (2529m 27s)\n",
      "epoch 80896, prm nrm: 1.4668, val acc: 0.5716, validation loss: 1.7768, training loss: 0.6927  15% (2545m 43s)\n",
      "epoch 81408, prm nrm: 2.5819, val acc: 0.5676, validation loss: 1.7745, training loss: 0.6871  15% (2562m 0s)\n",
      "epoch 81920, prm nrm: 2.5166, val acc: 0.5539, validation loss: 1.8330, training loss: 0.6986  16% (2578m 11s)\n",
      "epoch 82432, prm nrm: 1.3680, val acc: 0.5745, validation loss: 1.7918, training loss: 0.7053  16% (2594m 23s)\n",
      "epoch 82944, prm nrm: 1.9910, val acc: 0.5608, validation loss: 1.8285, training loss: 0.6978  16% (2610m 33s)\n",
      "epoch 83456, prm nrm: 2.0636, val acc: 0.5725, validation loss: 1.9461, training loss: 0.7022  16% (2626m 54s)\n",
      "epoch 83968, prm nrm: 1.1158, val acc: 0.5814, validation loss: 1.6855, training loss: 0.6871  16% (2643m 4s)\n",
      "epoch 84480, prm nrm: 1.7476, val acc: 0.5804, validation loss: 1.7801, training loss: 0.6772  16% (2659m 12s)\n",
      "epoch 84992, prm nrm: 1.2609, val acc: 0.5794, validation loss: 1.7000, training loss: 0.6879  16% (2675m 14s)\n",
      "epoch 85504, prm nrm: 0.8306, val acc: 0.5755, validation loss: 1.7233, training loss: 0.6793  16% (2691m 24s)\n",
      "epoch 86016, prm nrm: 9.1256, val acc: 0.5686, validation loss: 1.7801, training loss: 0.6670  16% (2707m 39s)\n",
      "epoch 86528, prm nrm: 0.9332, val acc: 0.5657, validation loss: 1.7658, training loss: 0.6681  16% (2723m 52s)\n",
      "epoch 87040, prm nrm: 1.0184, val acc: 0.5784, validation loss: 1.7481, training loss: 0.6970  17% (2739m 54s)\n",
      "epoch 87552, prm nrm: 1.1011, val acc: 0.5784, validation loss: 1.6885, training loss: 0.6819  17% (2756m 7s)\n",
      "epoch 88064, prm nrm: 1.2843, val acc: 0.5873, validation loss: 1.6954, training loss: 0.6773  17% (2772m 28s)\n",
      "epoch 88576, prm nrm: 1.4261, val acc: 0.5824, validation loss: 1.6787, training loss: 0.6923  17% (2789m 1s)\n",
      "epoch 89088, prm nrm: 1.5915, val acc: 0.5647, validation loss: 1.8556, training loss: 0.6766  17% (2805m 3s)\n",
      "epoch 89600, prm nrm: 1.6408, val acc: 0.5725, validation loss: 1.6080, training loss: 0.6715  17% (2821m 11s)\n",
      "epoch 90112, prm nrm: 1.0786, val acc: 0.5657, validation loss: 1.7994, training loss: 0.6635  17% (2837m 28s)\n",
      "epoch 90624, prm nrm: 2.4118, val acc: 0.5608, validation loss: 1.7599, training loss: 0.6609  17% (2853m 22s)\n",
      "epoch 91136, prm nrm: 1.2951, val acc: 0.5814, validation loss: 1.7106, training loss: 0.6851  17% (2869m 35s)\n",
      "epoch 91648, prm nrm: 2.0166, val acc: 0.5657, validation loss: 1.7399, training loss: 0.6628  17% (2885m 54s)\n",
      "epoch 92160, prm nrm: 1.6925, val acc: 0.5686, validation loss: 1.7239, training loss: 0.6692  18% (2902m 1s)\n",
      "epoch 92672, prm nrm: 1.3492, val acc: 0.5843, validation loss: 1.6581, training loss: 0.6544  18% (2918m 8s)\n",
      "epoch 93184, prm nrm: 2.8567, val acc: 0.5549, validation loss: 1.6489, training loss: 0.6733  18% (2934m 18s)\n",
      "epoch 93696, prm nrm: 3.4969, val acc: 0.5755, validation loss: 1.6813, training loss: 0.6470  18% (2950m 26s)\n",
      "epoch 94208, prm nrm: 2.4186, val acc: 0.5676, validation loss: 1.8411, training loss: 0.6609  18% (2966m 37s)\n",
      "epoch 94720, prm nrm: 1.3158, val acc: 0.5755, validation loss: 1.7451, training loss: 0.6693  18% (2982m 50s)\n",
      "epoch 95232, prm nrm: 1.0832, val acc: 0.5931, validation loss: 1.7836, training loss: 0.6598  18% (2998m 58s)\n",
      "epoch 95744, prm nrm: 2.5679, val acc: 0.5853, validation loss: 1.7462, training loss: 0.6539  18% (3015m 8s)\n",
      "epoch 96256, prm nrm: 2.3417, val acc: 0.5755, validation loss: 1.8602, training loss: 0.6659  18% (3031m 17s)\n",
      "epoch 96768, prm nrm: 1.9909, val acc: 0.5725, validation loss: 1.7831, training loss: 0.6443  18% (3047m 35s)\n",
      "epoch 97280, prm nrm: 3.0654, val acc: 0.5676, validation loss: 1.8161, training loss: 0.6566  19% (3063m 58s)\n",
      "epoch 97792, prm nrm: 0.9684, val acc: 0.5588, validation loss: 1.8848, training loss: 0.6557  19% (3080m 3s)\n",
      "epoch 98304, prm nrm: 1.7944, val acc: 0.5814, validation loss: 1.6841, training loss: 0.6627  19% (3096m 14s)\n",
      "epoch 98816, prm nrm: 1.8663, val acc: 0.5745, validation loss: 1.7441, training loss: 0.6482  19% (3112m 16s)\n",
      "epoch 99328, prm nrm: 2.1353, val acc: 0.5667, validation loss: 1.7620, training loss: 0.6438  19% (3128m 41s)\n",
      "epoch 99840, prm nrm: 1.2141, val acc: 0.5706, validation loss: 1.9478, training loss: 0.6388  19% (3144m 51s)\n",
      "epoch 100352, prm nrm: 1.7320, val acc: 0.5765, validation loss: 1.8064, training loss: 0.6649  19% (3161m 0s)\n",
      "epoch 100864, prm nrm: 1.4079, val acc: 0.5686, validation loss: 1.9455, training loss: 0.6260  19% (3176m 56s)\n",
      "epoch 101376, prm nrm: 1.7224, val acc: 0.5637, validation loss: 1.9401, training loss: 0.6354  19% (3193m 15s)\n",
      "epoch 101888, prm nrm: 1.3010, val acc: 0.5637, validation loss: 1.8452, training loss: 0.6371  19% (3209m 32s)\n",
      "epoch 102400, prm nrm: 1.0790, val acc: 0.5696, validation loss: 1.9135, training loss: 0.6201  20% (3225m 41s)\n",
      "epoch 102912, prm nrm: 1.3771, val acc: 0.5657, validation loss: 1.8733, training loss: 0.6316  20% (3241m 57s)\n",
      "epoch 103424, prm nrm: 2.0755, val acc: 0.5520, validation loss: 1.7388, training loss: 0.6266  20% (3258m 12s)\n",
      "epoch 103936, prm nrm: 1.6957, val acc: 0.5765, validation loss: 1.7753, training loss: 0.6223  20% (3274m 25s)\n",
      "epoch 104448, prm nrm: 1.1171, val acc: 0.5598, validation loss: 1.8316, training loss: 0.6349  20% (3290m 34s)\n",
      "epoch 104960, prm nrm: 1.1646, val acc: 0.5618, validation loss: 1.8123, training loss: 0.6397  20% (3306m 48s)\n",
      "epoch 105472, prm nrm: 1.1078, val acc: 0.5451, validation loss: 1.8824, training loss: 0.6193  20% (3323m 10s)\n",
      "epoch 105984, prm nrm: 1.5809, val acc: 0.5598, validation loss: 1.7970, training loss: 0.6312  20% (3339m 22s)\n",
      "epoch 106496, prm nrm: 2.7168, val acc: 0.5824, validation loss: 1.8372, training loss: 0.6206  20% (3355m 50s)\n",
      "epoch 107008, prm nrm: 1.6921, val acc: 0.5618, validation loss: 1.8492, training loss: 0.6208  20% (3372m 4s)\n",
      "epoch 107520, prm nrm: 1.4369, val acc: 0.5755, validation loss: 1.8606, training loss: 0.6076  21% (3388m 19s)\n",
      "epoch 108032, prm nrm: 1.4565, val acc: 0.5824, validation loss: 1.9109, training loss: 0.6132  21% (3404m 25s)\n",
      "epoch 108544, prm nrm: 1.7302, val acc: 0.5755, validation loss: 1.8498, training loss: 0.6195  21% (3420m 30s)\n",
      "epoch 109056, prm nrm: 1.4661, val acc: 0.5549, validation loss: 1.7645, training loss: 0.6233  21% (3436m 35s)\n",
      "epoch 109568, prm nrm: 1.3905, val acc: 0.5578, validation loss: 1.7744, training loss: 0.6183  21% (3452m 49s)\n",
      "epoch 110080, prm nrm: 2.2373, val acc: 0.5716, validation loss: 1.9122, training loss: 0.6108  21% (3468m 55s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 110592, prm nrm: 1.8729, val acc: 0.5794, validation loss: 1.9830, training loss: 0.6140  21% (3485m 1s)\n",
      "epoch 111104, prm nrm: 2.0650, val acc: 0.5657, validation loss: 1.9184, training loss: 0.6148  21% (3501m 15s)\n",
      "epoch 111616, prm nrm: 2.5532, val acc: 0.5598, validation loss: 1.9133, training loss: 0.6007  21% (3517m 38s)\n",
      "epoch 112128, prm nrm: 1.6458, val acc: 0.5569, validation loss: 1.7789, training loss: 0.6029  21% (3533m 54s)\n",
      "epoch 112640, prm nrm: 2.1075, val acc: 0.5686, validation loss: 1.9426, training loss: 0.6084  22% (3550m 7s)\n",
      "epoch 113152, prm nrm: 3.5633, val acc: 0.5735, validation loss: 1.9000, training loss: 0.6005  22% (3566m 23s)\n",
      "epoch 113664, prm nrm: 0.9734, val acc: 0.5706, validation loss: 1.8604, training loss: 0.5996  22% (3582m 43s)\n",
      "epoch 114176, prm nrm: 1.8983, val acc: 0.5824, validation loss: 1.9070, training loss: 0.6040  22% (3598m 52s)\n",
      "epoch 114688, prm nrm: 2.8834, val acc: 0.5618, validation loss: 1.9749, training loss: 0.5914  22% (3615m 8s)\n",
      "epoch 115200, prm nrm: 1.2886, val acc: 0.5647, validation loss: 1.8155, training loss: 0.5931  22% (3631m 23s)\n",
      "epoch 115712, prm nrm: 1.3166, val acc: 0.5549, validation loss: 2.0208, training loss: 0.5934  22% (3647m 41s)\n",
      "epoch 116224, prm nrm: 0.8599, val acc: 0.5716, validation loss: 1.9189, training loss: 0.5887  22% (3663m 54s)\n",
      "epoch 116736, prm nrm: 1.7373, val acc: 0.5775, validation loss: 1.8898, training loss: 0.5822  22% (3679m 54s)\n",
      "epoch 117248, prm nrm: 2.2195, val acc: 0.5735, validation loss: 1.9964, training loss: 0.5898  22% (3696m 6s)\n",
      "epoch 117760, prm nrm: 2.3124, val acc: 0.5667, validation loss: 1.9423, training loss: 0.5868  23% (3712m 14s)\n",
      "epoch 118272, prm nrm: 1.1906, val acc: 0.5775, validation loss: 1.8844, training loss: 0.5904  23% (3728m 24s)\n",
      "epoch 118784, prm nrm: 3.4039, val acc: 0.5716, validation loss: 1.9893, training loss: 0.5775  23% (3744m 36s)\n",
      "epoch 119296, prm nrm: 2.6485, val acc: 0.5676, validation loss: 1.9772, training loss: 0.5810  23% (3760m 35s)\n",
      "epoch 119808, prm nrm: 6.8585, val acc: 0.5794, validation loss: 1.8036, training loss: 0.6074  23% (3776m 44s)\n",
      "epoch 120320, prm nrm: 2.9162, val acc: 0.5676, validation loss: 2.0121, training loss: 0.5698  23% (3793m 1s)\n",
      "epoch 120832, prm nrm: 2.1962, val acc: 0.5735, validation loss: 1.9120, training loss: 0.5909  23% (3809m 1s)\n",
      "epoch 121344, prm nrm: 8.0449, val acc: 0.5814, validation loss: 1.8739, training loss: 0.5827  23% (3825m 25s)\n",
      "epoch 121856, prm nrm: 1.3652, val acc: 0.5794, validation loss: 1.9512, training loss: 0.5782  23% (3841m 27s)\n",
      "epoch 122368, prm nrm: 2.7133, val acc: 0.5471, validation loss: 1.9493, training loss: 0.5839  23% (3857m 43s)\n",
      "epoch 122880, prm nrm: 1.5858, val acc: 0.5706, validation loss: 1.8426, training loss: 0.5856  24% (3874m 2s)\n",
      "epoch 123392, prm nrm: 3.5133, val acc: 0.5706, validation loss: 1.9714, training loss: 0.5866  24% (3890m 6s)\n",
      "epoch 123904, prm nrm: 1.5243, val acc: 0.5775, validation loss: 1.9568, training loss: 0.5807  24% (3906m 31s)\n",
      "epoch 124416, prm nrm: 1.6976, val acc: 0.5520, validation loss: 2.0432, training loss: 0.5825  24% (3922m 47s)\n",
      "epoch 124928, prm nrm: 1.3145, val acc: 0.5520, validation loss: 2.0539, training loss: 0.5765  24% (3938m 57s)\n",
      "epoch 125440, prm nrm: 1.1269, val acc: 0.5490, validation loss: 1.9242, training loss: 0.5754  24% (3955m 10s)\n",
      "epoch 125952, prm nrm: 2.2649, val acc: 0.5657, validation loss: 1.9384, training loss: 0.5781  24% (3971m 21s)\n",
      "epoch 126464, prm nrm: 1.0204, val acc: 0.5520, validation loss: 1.9933, training loss: 0.5583  24% (3987m 25s)\n",
      "epoch 126976, prm nrm: 1.9480, val acc: 0.5500, validation loss: 2.0980, training loss: 0.5636  24% (4003m 43s)\n",
      "epoch 127488, prm nrm: 1.7334, val acc: 0.5745, validation loss: 1.9950, training loss: 0.5654  24% (4019m 47s)\n",
      "epoch 128000, prm nrm: 1.1986, val acc: 0.5657, validation loss: 1.8674, training loss: 0.5721  25% (4036m 9s)\n",
      "epoch 128512, prm nrm: 2.3140, val acc: 0.5578, validation loss: 1.9054, training loss: 0.5730  25% (4052m 16s)\n",
      "epoch 129024, prm nrm: 1.8686, val acc: 0.5657, validation loss: 1.9094, training loss: 0.5808  25% (4068m 29s)\n",
      "epoch 129536, prm nrm: 1.7183, val acc: 0.5520, validation loss: 2.0181, training loss: 0.5623  25% (4084m 42s)\n",
      "epoch 130048, prm nrm: 4.4478, val acc: 0.5657, validation loss: 2.0014, training loss: 0.5672  25% (4100m 50s)\n",
      "epoch 130560, prm nrm: 1.4366, val acc: 0.5637, validation loss: 1.8378, training loss: 0.5641  25% (4117m 6s)\n",
      "epoch 131072, prm nrm: 1.9018, val acc: 0.5637, validation loss: 1.9745, training loss: 0.5767  25% (4133m 10s)\n",
      "epoch 131584, prm nrm: 1.6930, val acc: 0.5480, validation loss: 2.1131, training loss: 0.5614  25% (4149m 31s)\n",
      "epoch 132096, prm nrm: 0.8976, val acc: 0.5549, validation loss: 2.0827, training loss: 0.5581  25% (4165m 40s)\n",
      "epoch 132608, prm nrm: 1.3997, val acc: 0.5647, validation loss: 2.0802, training loss: 0.5575  25% (4181m 34s)\n",
      "epoch 133120, prm nrm: 2.1276, val acc: 0.5480, validation loss: 2.0550, training loss: 0.5634  26% (4197m 53s)\n",
      "epoch 133632, prm nrm: 1.6606, val acc: 0.5569, validation loss: 2.0549, training loss: 0.5494  26% (4214m 1s)\n",
      "epoch 134144, prm nrm: 1.5751, val acc: 0.5676, validation loss: 1.9755, training loss: 0.5457  26% (4230m 2s)\n",
      "epoch 134656, prm nrm: 1.5954, val acc: 0.5735, validation loss: 2.1042, training loss: 0.5491  26% (4246m 15s)\n",
      "epoch 135168, prm nrm: 1.3297, val acc: 0.5627, validation loss: 1.8834, training loss: 0.5545  26% (4262m 36s)\n",
      "epoch 135680, prm nrm: 2.4160, val acc: 0.5559, validation loss: 2.0030, training loss: 0.5451  26% (4278m 48s)\n",
      "epoch 136192, prm nrm: 1.0943, val acc: 0.5539, validation loss: 2.0437, training loss: 0.5469  26% (4294m 45s)\n",
      "epoch 136704, prm nrm: 2.4103, val acc: 0.5667, validation loss: 2.0401, training loss: 0.5384  26% (4310m 47s)\n",
      "epoch 137216, prm nrm: 1.6790, val acc: 0.5578, validation loss: 2.0259, training loss: 0.5490  26% (4326m 48s)\n",
      "epoch 137728, prm nrm: 3.0042, val acc: 0.5657, validation loss: 1.8635, training loss: 0.5348  26% (4342m 52s)\n",
      "epoch 138240, prm nrm: 2.1765, val acc: 0.5539, validation loss: 2.0872, training loss: 0.5417  27% (4358m 59s)\n",
      "epoch 138752, prm nrm: 0.8272, val acc: 0.5559, validation loss: 1.9532, training loss: 0.5408  27% (4375m 7s)\n",
      "epoch 139264, prm nrm: 2.6109, val acc: 0.5578, validation loss: 2.0270, training loss: 0.5382  27% (4391m 4s)\n",
      "epoch 139776, prm nrm: 2.0471, val acc: 0.5353, validation loss: 2.1219, training loss: 0.5339  27% (4407m 12s)\n",
      "epoch 140288, prm nrm: 2.2423, val acc: 0.5451, validation loss: 2.0686, training loss: 0.5415  27% (4423m 28s)\n",
      "epoch 140800, prm nrm: 1.5295, val acc: 0.5510, validation loss: 2.0889, training loss: 0.5416  27% (4439m 42s)\n",
      "epoch 141312, prm nrm: 2.7036, val acc: 0.5392, validation loss: 2.0895, training loss: 0.5334  27% (4455m 57s)\n",
      "epoch 141824, prm nrm: 1.9256, val acc: 0.5451, validation loss: 1.9961, training loss: 0.5411  27% (4472m 3s)\n",
      "epoch 142336, prm nrm: 2.1938, val acc: 0.5402, validation loss: 2.0399, training loss: 0.5423  27% (4488m 25s)\n",
      "epoch 142848, prm nrm: 2.2358, val acc: 0.5765, validation loss: 2.0024, training loss: 0.5375  27% (4504m 48s)\n",
      "epoch 143360, prm nrm: 3.0481, val acc: 0.5500, validation loss: 2.0748, training loss: 0.5346  28% (4520m 59s)\n",
      "epoch 143872, prm nrm: 2.9190, val acc: 0.5618, validation loss: 1.9778, training loss: 0.5331  28% (4537m 13s)\n",
      "epoch 144384, prm nrm: 2.1749, val acc: 0.5422, validation loss: 2.1050, training loss: 0.5320  28% (4553m 22s)\n",
      "epoch 144896, prm nrm: 1.3581, val acc: 0.5539, validation loss: 1.9763, training loss: 0.5446  28% (4569m 20s)\n",
      "epoch 145408, prm nrm: 2.0379, val acc: 0.5471, validation loss: 2.1377, training loss: 0.5326  28% (4585m 44s)\n",
      "epoch 145920, prm nrm: 2.6671, val acc: 0.5569, validation loss: 2.1194, training loss: 0.5291  28% (4601m 59s)\n",
      "epoch 146432, prm nrm: 1.8419, val acc: 0.5559, validation loss: 2.1105, training loss: 0.5168  28% (4618m 15s)\n",
      "epoch 146944, prm nrm: 1.4117, val acc: 0.5431, validation loss: 2.0920, training loss: 0.5203  28% (4634m 37s)\n",
      "epoch 147456, prm nrm: 1.3666, val acc: 0.5480, validation loss: 2.0386, training loss: 0.5232  28% (4650m 37s)\n",
      "epoch 147968, prm nrm: 1.5530, val acc: 0.5647, validation loss: 2.0932, training loss: 0.5276  28% (4666m 44s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 148480, prm nrm: 1.4419, val acc: 0.5647, validation loss: 1.9572, training loss: 0.5255  28% (4682m 48s)\n",
      "epoch 148992, prm nrm: 1.1007, val acc: 0.5588, validation loss: 1.9809, training loss: 0.5177  29% (4698m 59s)\n",
      "epoch 149504, prm nrm: 2.5484, val acc: 0.5569, validation loss: 2.0415, training loss: 0.5203  29% (4715m 19s)\n",
      "epoch 150016, prm nrm: 3.5772, val acc: 0.5500, validation loss: 2.1197, training loss: 0.5167  29% (4731m 40s)\n",
      "epoch 150528, prm nrm: 2.0873, val acc: 0.5549, validation loss: 2.0942, training loss: 0.5136  29% (4747m 57s)\n",
      "epoch 151040, prm nrm: 2.3489, val acc: 0.5490, validation loss: 2.1542, training loss: 0.5059  29% (4764m 1s)\n",
      "epoch 151552, prm nrm: 1.9370, val acc: 0.5618, validation loss: 2.0594, training loss: 0.5086  29% (4780m 10s)\n",
      "epoch 152064, prm nrm: 1.8386, val acc: 0.5471, validation loss: 1.9171, training loss: 0.5304  29% (4796m 13s)\n",
      "epoch 152576, prm nrm: 3.0540, val acc: 0.5441, validation loss: 1.9665, training loss: 0.5267  29% (4812m 32s)\n",
      "epoch 153088, prm nrm: 1.8947, val acc: 0.5549, validation loss: 2.1577, training loss: 0.5109  29% (4828m 48s)\n",
      "epoch 153600, prm nrm: 2.4638, val acc: 0.5657, validation loss: 1.9341, training loss: 0.5202  30% (4845m 4s)\n",
      "epoch 154112, prm nrm: 2.0907, val acc: 0.5539, validation loss: 2.0811, training loss: 0.5100  30% (4861m 15s)\n",
      "epoch 154624, prm nrm: 1.7525, val acc: 0.5549, validation loss: 2.0566, training loss: 0.4894  30% (4877m 28s)\n",
      "epoch 155136, prm nrm: 0.8781, val acc: 0.5529, validation loss: 2.0599, training loss: 0.5202  30% (4893m 35s)\n",
      "epoch 155648, prm nrm: 1.7612, val acc: 0.5598, validation loss: 2.0081, training loss: 0.5059  30% (4909m 49s)\n",
      "epoch 156160, prm nrm: 2.3741, val acc: 0.5608, validation loss: 2.2004, training loss: 0.5092  30% (4925m 53s)\n",
      "epoch 156672, prm nrm: 2.1262, val acc: 0.5500, validation loss: 2.0540, training loss: 0.5080  30% (4942m 8s)\n",
      "epoch 157184, prm nrm: 1.3321, val acc: 0.5373, validation loss: 2.1179, training loss: 0.5195  30% (4958m 21s)\n",
      "epoch 157696, prm nrm: 1.2488, val acc: 0.5490, validation loss: 2.0075, training loss: 0.5013  30% (4974m 36s)\n",
      "epoch 158208, prm nrm: 2.8546, val acc: 0.5471, validation loss: 2.1322, training loss: 0.4938  30% (4990m 35s)\n",
      "epoch 158720, prm nrm: 2.9736, val acc: 0.5559, validation loss: 2.0942, training loss: 0.5058  31% (5006m 34s)\n",
      "epoch 159232, prm nrm: 2.2710, val acc: 0.5598, validation loss: 2.0581, training loss: 0.5157  31% (5022m 28s)\n",
      "epoch 159744, prm nrm: 3.1152, val acc: 0.5304, validation loss: 2.2898, training loss: 0.4822  31% (5038m 48s)\n",
      "epoch 160256, prm nrm: 1.3339, val acc: 0.5422, validation loss: 1.9657, training loss: 0.5045  31% (5055m 4s)\n",
      "epoch 160768, prm nrm: 1.3740, val acc: 0.5333, validation loss: 2.0532, training loss: 0.5017  31% (5071m 19s)\n",
      "epoch 161280, prm nrm: 3.8140, val acc: 0.5647, validation loss: 2.1214, training loss: 0.4992  31% (5087m 27s)\n",
      "epoch 161792, prm nrm: 2.1210, val acc: 0.5598, validation loss: 2.2020, training loss: 0.5125  31% (5103m 46s)\n",
      "epoch 162304, prm nrm: 2.3089, val acc: 0.5578, validation loss: 2.0688, training loss: 0.4937  31% (5120m 2s)\n",
      "epoch 162816, prm nrm: 2.2687, val acc: 0.5725, validation loss: 1.9628, training loss: 0.4869  31% (5136m 26s)\n",
      "epoch 163328, prm nrm: 5.1145, val acc: 0.5676, validation loss: 2.0849, training loss: 0.4918  31% (5152m 45s)\n",
      "epoch 163840, prm nrm: 1.7215, val acc: 0.5608, validation loss: 2.0888, training loss: 0.4849  32% (5168m 49s)\n",
      "epoch 164352, prm nrm: 1.3892, val acc: 0.5569, validation loss: 2.0830, training loss: 0.4852  32% (5185m 5s)\n",
      "epoch 164864, prm nrm: 2.5206, val acc: 0.5500, validation loss: 2.1598, training loss: 0.4931  32% (5201m 10s)\n",
      "epoch 165376, prm nrm: 0.9822, val acc: 0.5539, validation loss: 2.0899, training loss: 0.4926  32% (5217m 8s)\n",
      "epoch 165888, prm nrm: 1.3846, val acc: 0.5578, validation loss: 2.0951, training loss: 0.4981  32% (5233m 18s)\n",
      "epoch 166400, prm nrm: 1.1127, val acc: 0.5569, validation loss: 2.0683, training loss: 0.4838  32% (5249m 22s)\n",
      "epoch 166912, prm nrm: 1.2948, val acc: 0.5686, validation loss: 2.1878, training loss: 0.5012  32% (5265m 26s)\n",
      "epoch 167424, prm nrm: 1.8940, val acc: 0.5373, validation loss: 2.1104, training loss: 0.4914  32% (5281m 26s)\n",
      "epoch 167936, prm nrm: 1.7521, val acc: 0.5647, validation loss: 2.1503, training loss: 0.4888  32% (5297m 27s)\n",
      "epoch 168448, prm nrm: 3.3072, val acc: 0.5549, validation loss: 2.2125, training loss: 0.4895  32% (5313m 50s)\n",
      "epoch 168960, prm nrm: 3.3678, val acc: 0.5490, validation loss: 2.2008, training loss: 0.4918  33% (5330m 2s)\n",
      "epoch 169472, prm nrm: 6.4825, val acc: 0.5784, validation loss: 2.1146, training loss: 0.4822  33% (5346m 27s)\n",
      "epoch 169984, prm nrm: 2.4721, val acc: 0.5353, validation loss: 2.1182, training loss: 0.4828  33% (5362m 33s)\n",
      "epoch 170496, prm nrm: 2.3081, val acc: 0.5500, validation loss: 2.1661, training loss: 0.4933  33% (5378m 37s)\n",
      "epoch 171008, prm nrm: 4.0160, val acc: 0.5676, validation loss: 2.0482, training loss: 0.4898  33% (5395m 0s)\n",
      "epoch 171520, prm nrm: 2.5503, val acc: 0.5578, validation loss: 2.1974, training loss: 0.4749  33% (5411m 5s)\n",
      "epoch 172032, prm nrm: 1.6005, val acc: 0.5598, validation loss: 2.0996, training loss: 0.4863  33% (5427m 28s)\n",
      "epoch 172544, prm nrm: 2.2930, val acc: 0.5578, validation loss: 2.0780, training loss: 0.4747  33% (5443m 41s)\n",
      "epoch 173056, prm nrm: 2.6080, val acc: 0.5588, validation loss: 2.0563, training loss: 0.4647  33% (5459m 59s)\n",
      "epoch 173568, prm nrm: 1.9551, val acc: 0.5490, validation loss: 2.1382, training loss: 0.4787  33% (5476m 9s)\n",
      "epoch 174080, prm nrm: 2.1751, val acc: 0.5696, validation loss: 2.1221, training loss: 0.4651  34% (5492m 26s)\n",
      "epoch 174592, prm nrm: 4.1782, val acc: 0.5716, validation loss: 2.1009, training loss: 0.4817  34% (5508m 36s)\n",
      "epoch 175104, prm nrm: 2.0730, val acc: 0.5490, validation loss: 2.2302, training loss: 0.4722  34% (5524m 50s)\n",
      "epoch 175616, prm nrm: 2.1919, val acc: 0.5314, validation loss: 2.3094, training loss: 0.4579  34% (5541m 12s)\n",
      "epoch 176128, prm nrm: 2.1879, val acc: 0.5451, validation loss: 2.1396, training loss: 0.4683  34% (5557m 22s)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-0fa98e7b0388>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcategory_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_training_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocabClass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m#output, loss= train2(category_tensor, input_var, rnn, clip = clip)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcurrent_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-7c505b5b72bd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(category_tensor, line_tensor, rnn, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_rep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_wts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;31m#total_norm = max(p.grad.data.abs().max() for p in rnn.parameters())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m#print(total_norm.cpu().numpy())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "    category_tensor, input_var, input_lengths = random_training_batch(batch_size,vocabClass)\n",
    "    output, loss, param_norm = train(category_tensor, input_var, rnn, max_norm = clip, norm_type = norm_type)\n",
    "    #output, loss= train2(category_tensor, input_var, rnn, clip = clip)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if epoch % plot_every == 0:\n",
    "        avg_val_loss, val_accuracy = eval_dict(vocabClass,validation_dict)\n",
    "        current_vloss += avg_val_loss\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            print('saving model...')\n",
    "            save_model(rnn,name)\n",
    "            print('avg_val_loss %.4f ,  val_accuracy %.4f' % (avg_val_loss, val_accuracy))\n",
    "            \n",
    "        print('epoch %d, prm nrm: %.4f, val acc: %.4f, validation loss: %.4f, training loss: %.4f  %d%% (%s)' % (epoch, \n",
    "               param_norm, val_accuracy, avg_val_loss,\n",
    "               current_loss / plot_every, epoch / n_epochs * 100, time_since(start)))\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        all_vlosses.append(current_vloss)\n",
    "        current_loss = 0\n",
    "        current_vloss = 0\n",
    "        \n",
    "    scheduler.step()\n",
    "    \n",
    "#avg_val_loss, val_accuracy = eval_dict(vocabClass,validation_dict)\n",
    "#print('avg_val_loss %.4f ,  val_accuracy %.4f' % (avg_val_loss, val_accuracy)) \n",
    "plt.figure()\n",
    "plt.xlabel('Epochs', fontsize=25)\n",
    "plt.ylabel('Cross Entropy Loss', fontsize=25)\n",
    "plt.plot(all_losses, label='train')\n",
    "plt.plot(all_vlosses, label='val')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEfCAYAAACEbivCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1d58360160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.xlabel('Epochs', fontsize=25)\n",
    "plt.ylabel('Cross Entropy Loss', fontsize=25)\n",
    "plt.plot(all_losses, label='train')\n",
    "plt.plot(all_vlosses, label='val')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAysAAALvCAYAAABlWVpKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1EsGK5IkSZJ66f8Dj0hjzjHfYdcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5df43051d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def category_from_output(output):\n",
    "    top_n, top_i = output.data.topk(1) # Tensor out of Variable with .data\n",
    "    category_i = top_i[0][0]\n",
    "    return all_categories[category_i], category_i\n",
    "\n",
    "def random_training_pair(category_lines):                                                                                                               \n",
    "    category = random.choice(all_categories)\n",
    "    line = random.choice(category_lines[category])\n",
    "    category_tensor = Variable(torch.LongTensor([all_categories.index(category)]))\n",
    "    line_tensor = Variable(torch.LongTensor(indexesFromSentence(vocabClass, line))).view(-1,1)\n",
    "    if use_cuda:\n",
    "        line_tensor = line_tensor.cuda()\n",
    "        category_tensor = category_tensor.cuda()\n",
    "    return category, line, category_tensor, line_tensor\n",
    "\n",
    "# Keep track of correct guesses in a confusion matrix\n",
    "confusion = torch.zeros(n_categories, n_categories)\n",
    "n_confusion = 1000\n",
    "\n",
    "# Just return an output given a line\n",
    "def evaluate(line_tensor):\n",
    "    rnn.train(False)\n",
    "    output, rep, attn_wts = rnn(line_tensor)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Go through a bunch of examples and record which are correctly guessed\n",
    "data_dict = test_dict\n",
    "for i in range(n_confusion):\n",
    "    category, line, category_tensor, line_tensor = random_training_pair(data_dict)\n",
    "    output = evaluate(line_tensor)\n",
    "    guess, guess_i = category_from_output(output)\n",
    "    category_i = all_categories.index(category)\n",
    "    confusion[category_i][guess_i] += 1\n",
    "\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(n_categories):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure()\n",
    "plt.rcParams['figure.figsize'] = [12,12]\n",
    "plt.rcParams['axes.labelsize'] = 'large'\n",
    "plt.rcParams['xtick.labelsize']=18\n",
    "plt.rcParams['ytick.labelsize']=18\n",
    "plt.xlabel('Prediction', fontsize=25)\n",
    "plt.ylabel('Truth', fontsize=25)\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion.numpy())\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels([''] + all_categories, rotation=90)\n",
    "ax.set_yticklabels([''] + all_categories)\n",
    "\n",
    "# Force label at every tick\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix: actual language (rows) which language the network guesses (columns)\n",
    "\n",
    "fig.savefig(\"confusionMatrix_v13.png\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_val_loss 1.2933 ,  val_accuracy 0.5933\n"
     ]
    }
   ],
   "source": [
    "avg_loss, accuracy = eval_dict(vocabClass,test_dict)\n",
    "print('avg_val_loss %.4f ,  val_accuracy %.4f' % (avg_loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_line(line, vocabClass, rnn):\n",
    "        line_tensor = Variable(torch.LongTensor(indexesFromSentence(vocabClass, line))).view(-1,1)\n",
    "        if use_cuda:\n",
    "            line_tensor = line_tensor.cuda()\n",
    "        rnn.train(False)\n",
    "        output, rep, attn_wts = rnn(line_tensor)\n",
    "        guess, guess_i = category_from_output(output)\n",
    "        return guess, output, attn_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depression\n",
      "[[-1.5660154   1.3337743  -0.6406571  -0.97600603 -0.4812172   0.62176216\n",
      "   0.32732072 -0.5785853 ]]\n",
      "[[[0.00958077 0.00649086 0.01840404 0.00221633 0.09877513 0.12239128\n",
      "   0.12400452 0.08030707 0.53783   ]]]\n",
      "i dont enjoy the things i used to\n"
     ]
    }
   ],
   "source": [
    "#string = \" I am not happy, I cannot feel joy, I havent been optimistic, I dont enjoy the things I used to \"\n",
    "string = \" I am not happy\" #\n",
    "string = \" I dont enjoy the things I used to \"\n",
    "prediction, output, attention_weights = predict_line(string, vocabClass, rnn)\n",
    "print(prediction)\n",
    "print(output.cpu().detach().numpy())\n",
    "print(attention_weights.cpu().detach().numpy())\n",
    "print(vocabClass.normalize_string(string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_data(data_dict,lang):\n",
    "    \n",
    "    emotype = {}\n",
    "    index = 0\n",
    "    \n",
    "    for category in data_dict.keys():\n",
    "        category_tensor = Variable(torch.LongTensor([all_categories.index(category)]))\n",
    "        if use_cuda:\n",
    "            category_tensor = category_tensor.cuda()\n",
    "        for line in  data_dict[category]:\n",
    "            emotype[index] = {}\n",
    "            emotype[index]['text'] = line\n",
    "            emotype[index]['text_tokens'] = vocabClass.normalize_string(line)\n",
    "            emotype[index]['label'] = category\n",
    "            line_tensor = Variable(torch.LongTensor(indexesFromSentence(lang, line))).view(-1,1)\n",
    "            if use_cuda:\n",
    "                line_tensor = line_tensor.cuda()\n",
    "            output, vector_rep, attention_weights = rnn(line_tensor)\n",
    "            emotype[index]['attention_weights'] = attention_weights.cpu().detach().numpy()\n",
    "            attn_weights = attention_weights.cpu().detach().numpy()[0][0][:-1]\n",
    "            attn_weights = np.clip(attn_weights,0.0,2*np.mean(attn_weights))\n",
    "            attn_weights /= np.sum(attn_weights)\n",
    "            emotype[index]['rescaled_attention_weights'] = attn_weights\n",
    "            emotype[index]['encoding'] = vector_rep.data.cpu().numpy()\n",
    "            topv, topi = output.data.topk(1, 1, True)\n",
    "            category_index = topi[0][0]\n",
    "            emotype[index]['prediction'] = all_categories[category_index]\n",
    "            emotype[index]['outputs'] = output.data.cpu().numpy()\n",
    "\n",
    "            index += 1\n",
    "\n",
    "    return emotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "emotype = encode_data(validation_dict,vocabClass)\n",
    "#emotype = eval_validation_vectors(category_lines_validation)\n",
    "pickle.dump(emotype, open(\"text_encoding/emotype_v14_valset.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def turn_attn_weight_into_color(weight, tmp_cmap):\n",
    "    color_indx = int(255 * weight)\n",
    "    r, g, b, alpha = [int(255 * tmp_cmap(color_indx)[i]) for i in range(4)]\n",
    "    return(r, g, b)\n",
    "\n",
    "def get_word_with_rgb(word, r, g, b):\n",
    "    esc = \"\\x1b[\"\n",
    "    txt_style = \"2;\"  # Text style (\"1\" is bold, \"2\" is not bold)\n",
    "    toggle_bg = \"48;2;\"  # Switch to toggle background color (38;2; is foreground color)\n",
    "    r = str(r)  # Red\n",
    "    g = str(g)  # Green\n",
    "    b = str(b)  # Blue\n",
    "    ansi_code = esc + txt_style + toggle_bg + r + \";\" + g + \";\" + b + \"m\"\n",
    "    return(\"%s%s\" % (ansi_code, word))\n",
    "\n",
    "def category_from_output(output):\n",
    "    top_n, top_i = output.data.topk(1) # Tensor out of Variable with .data\n",
    "    category_i = top_i[0][0]\n",
    "    return all_categories[category_i], category_i\n",
    "\n",
    "def predict_line(line, vocabClass, rnn):\n",
    "        rnn.train(False)\n",
    "        op_txt = ''\n",
    "        my_cmap = plt.cm.get_cmap(name='Reds')\n",
    "        tokens = re.split(' ', vocabClass.normalize_string(line))\n",
    "        line_tensor = Variable(torch.LongTensor(indexesFromSentence(vocabClass, line))).view(-1,1)\n",
    "        if use_cuda:\n",
    "            line_tensor = line_tensor.cuda()\n",
    "        output, rep, attn_wts = rnn(line_tensor)\n",
    "        attn_weights = attn_wts.cpu().detach().numpy()[0][0][:-1]\n",
    "        attn_weights = np.clip(attn_weights,0.0,2*np.mean(attn_weights))\n",
    "        attn_weights /= np.sum(attn_weights)\n",
    "        #print(attn_weights)\n",
    "        guess, guess_i = category_from_output(output)\n",
    "        for word_indx, word in enumerate(tokens):\n",
    "            r, g, b = turn_attn_weight_into_color(attn_weights[word_indx], my_cmap)\n",
    "            op_txt += get_word_with_rgb(word, r, g, b) + ' '\n",
    "        # Note that we have to explicitly set the normal background to plain ol' white\n",
    "        print('prediction: ', guess)\n",
    "        print(op_txt + '\\n')\n",
    "        \n",
    "        return guess, output, attn_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction:  selfharm\n",
      "\u001b[2;48;2;254;239;231mi \u001b[2;48;2;254;243;237mhave \u001b[2;48;2;254;239;232monly \u001b[2;48;2;253;219;203mmyself \u001b[2;48;2;252;153;122mto \u001b[2;48;2;251;134;102mblame \n",
      "\n"
     ]
    }
   ],
   "source": [
    "string = \" i have only myself to blame\" \n",
    "\n",
    "prediction, output, attention_weights = predict_line(string, vocabClass, rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction:  happy\n",
      "\u001b[2;48;2;254;238;230mi \u001b[2;48;2;254;242;236mhave \u001b[2;48;2;254;242;236mdiscovered \u001b[2;48;2;254;240;233mmy \u001b[2;48;2;253;221;206mlifes \u001b[2;48;2;251;144;112mpurpose \u001b[2;48;2;251;144;112m! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "string = \" i have discovered my life's purpose  \" \n",
    "\n",
    "prediction, output, attention_weights = predict_line(string, vocabClass, rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction:  happy\n",
      "\u001b[2;48;2;251;135;103mi \u001b[2;48;2;252;172;144mhave \u001b[2;48;2;252;172;144mfriends \n",
      "\n"
     ]
    }
   ],
   "source": [
    "string = \"  i have  friends\"  \n",
    "\n",
    "prediction, output, attention_weights = predict_line(string, vocabClass, rnn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
